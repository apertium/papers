*Pending: *

Statistics:
- 4- The standard deviation is very high. This is probably due to the lack of guidelines and training for the annotators. Or the reason can be the subjectivity of the task. It should be useful to perform some statistical significance tests (such as t-test, ANOVA) to evaluate whether the differences between each set-up are really significant or happened by chance. It is difficult to make any assumption without these tests, mainly because the annotators did not receiving any training.
- 14 * It would be great to see statistical significance testing. [JAPREZ: all these comments say basically the same; you can leave the tables as they are but if you have time you can perform pairwise significance t tests; see, for example, http://www.stat.yale.edu/Courses/1997-98/101/sigtest.htm and http://reference.wolfram.com/language/ref/TTest.html]
- 22 There are several aspects of the analysis that are not clear,
but what it looks like is that the annotator agreement on the task is very low, and the reasons for this
should be explored. [JAPEREZ: the paper states the oppsite: that inter-annotator agreement is relatively high; the reviewer is wrong here but probably you should check the paper trying to find a possible source for this missunderstanding]
- 26 Firstly, you should show confidence intervals, not standard deviation,
and this requires knowing how many evaluators are in each condition.
- 27 Secondly, the standard deviations are huge. If I
see a figure like 62.19 +/- 32, then taking this as a confidence interval means potentially a 50% relative error! There
is no sense in reporting 2 decimal places with such large errors.  [JAPEREZ: remove decimals]

Add/remove large sections:

Katya: looks like we only have space to add things if we remove something. Please tell me which of these you would like to see.

- 7- Section 4.3 should not appear. It does not add any substantial information. [JAPEREZ: I agree completely]
- 10- A section of related work is also missing (could be included in the place of Section 4.3).   
Katya: should it be something other than the articles we mention in the introduction? [JAPEREZ: IMO we are focusing on assimilation evaluation; that's why we don't discuss other forms of evaluation; you could add a single sentence explaining this to the introduction and refer the reader to chapter 8 of Koehn's book (get the reference from http://statmt.org/book/) for a deeper discussion on evaluation]
- 32 Finally I felt that the brief literature review gave a rather skewed view of MT evaluation. You could, for instance,
mention the large-scale efforts of NIST, IWSLT and WMT who have experimented with different human evaluation measures.
Also relevant is work on monolingual post-editing such as from Rebecca Hwa and Philipp Koehn.  [JAPEREZ: same as before, but to make the reviewers happy you could mention these conferences and also the paper; BTW, we do not necessarily need an explicit section with related work; it can be in the introduction as it is now]
- 28 Also in section 4, you mention timing information. This is important, so why not show it? You have space. [JAPEREZ: we don't have space, but you could add a couple of sentences with some of the times]

Additional info:

Katya: these are the suggestions that require extra research

- 9- An interesting analysis should be correlating the results of gap filling evaluation method with traditional evaluation metrics or human evaluation. [JAPEREZ: add to future work]
- 15 * I am interested how good the MT translation was. Maybe, the authors could provide BLEU scores and some examples. [JAPEREZ: BLEU could be added (if you have time) for illustrative purposes but note that, in our case, we do not consider BLEU as a good measure of MT quality in the context of assimilation. We should make this clear.]
- 3- Did you consider including a manual evaluation of the keyword removal automatic task? This could tell the quality of the designed tasks.
- 23 The abstract mentions that "The evaluation results show that the gap-filling task reflects users' understanding
of the text, and may be used to measure MT quality for assimilation purposes". Do you really believe this? How
did you correlate the task with the users' understanding - if that is really possible? How do you use it for measuring
MT quality - it's a step in that direction but you haven't shown how to turn it into a quality measure. [JAPEREZ: the tone of the sentence should be lessen.] 


*Has comments in article:*

12 * The participants are native speakers of a target language and have no command of a source language. How did you check that the speakers were monolingual speakers? Did they complete some tests? For example, many Russian people living in Tatarstan and attending obligatory Tatar lessons at school at least understand some primitive expressions in Tatar. I suppose that Kazakh participants could have English as their foreign language. It would be also interesting to know, whether the participants were similar to each other (in age, university degree, number of known foreign languages). [JAPEREZ: you can provide more details about this]

30 In 4.1 the annotator agreement is presented, but no reasons for the low values are suggested. In fact the poor level
of agreement is completely ignored, and the only comment is about a high level of agreement (I can't see 0.917 in the
table though). [JAPEREZ: yes: where is the 0.917 in the table?] Katya: it is not. I looked up alphas by domains, trying to see which contributed the most, and it is from the news domain. It is the last table, table 28 in Tatar--Russian results. 

31 Krippendorff's alpha is perhaps not that familiar to the readership (I had to look it up) so you could
include some brief guidelines on how to interpret it.
Katya: there isn't really an agreed interpretation, I found one widely cited in Landis and Koch 1977, will check and insert [JAPEREZ: ok]


*Has questions:*

1- The authors should state on the introduction how they will use human references gap filling to evaluate MT. This is unclear until you arrive in Section 2. A short sentences stating it should help.
Katya: we say something at the end of paragraph 2 of the introduction, should I rephrase? [JAPEREZ: explain briefly how the gap-filling method works]

2- "n" was defined empirically. Did you consider other ways to find this "n"?
Katya: what is "n"?

11 * The evaluation technique is interesting, however, it requires a human translation, a human for gap filling, and a human for evaluating the gap filling. This seems to be quite costly. [JAPEREZ: the first two kind of humans are also needed in "standard" evaluations such as scoring; we use the third one only for synonyms but it is not mandatory]
16 * The paper could be improved by giving examples for wrongly filled gaps and an error analysis.
Katya: we say about parallel corpora and automatic checking in sections 3 and 3.1, should we expand/clarify? [JAPEREZ: error analysis could be interesting but we do not have space]

17 * Which points are innovative in comparison to the approach of O'Regan and Forcada (2013)?
Katya: 10 and 30% gaps, automatic generation, something else? [JAPEREZ: it is already said, right? Make sure it is clear in the text.]

*Solved:*

5- The discussion is a bit confusing. On the fifth paragraph of Section 4, the authors say that the MT+source 10% sentences in Tatar-Russian scored lower than the corresponding MT task. However, the values presented on Table 6 show a different scenario (57% for MT and 63% for MT+source).
Katya: mixed table columns. fixed and checked the others.

6- On the discussing of Section 4.2, again the same confusion with numbers happen. The authors should review this statements. The author also say the the improvements were significantly, although the majority of percentages between MT and MT+source are very similar (considering the high standard deviation, it is also not clear).
Katya: rephrased

8- Table 5 and 6 should be 3 and 4, since they are mentioned earlier in the paper. [JAPEREZ: already fixed?]

29 Were they paid for the work? (that could help!) Is the task better
or worse than other MT evaluation methods? The task doesn't sound too bad to me, so why exactly did the participants
not like it?
Katya: expanded the paragraph immediately before section 4.1

13 * The participants have solved the MT+source 10% tasks at first. Was the task sequence for all participants always the same? A better way might be to change the order of tasks.
Katya: yes, we write about this

18. Proper noun is not a part of speech, it is only a class of nouns (which also includes common nouns). [JAPEREZ: fix this]


19. It is mentioned that common language and language intuition can be used to for gap filling of idioms and strong collocations. But this point was not taken into account by gap creation and I couldn't find any examples.
Katya: Added an example to "reference sentence only"

20. I did not understand the step in keyword preparation where the next word is considered as a candidate when the word has already been removed. Does this mean that two successive gaps are possible?
Katya: uhm, yes, looks like it.  [JAPREZ: clarify]

21. A native speaker of a target language checked synonym lists. It is not clear whether it was an independent reviewer or one of the participants. In my opinion, he/she should also understand a source sentence to evaluate the appropriateness of a replacement.
Katya: clarified that they were not a participant

24 With 11 evaluators (Basque-Spanish), each covering 36 sentences, and 12
conditions (3x4) surely you average less than 1 per condition per sentence? I must be missing something.
Katya: tried to rephrase that section, not sure if it made it more clear.

25 Also in 3.1, you
 say that (in relation to synonyms) "scores improved by three per cent" - do you mean 3 percentage points, or 3% relative?
 Katya: wrote "percentage points"

