%
% File eamt15.tex, an adaptation of eamt14.tex (which was a copy of
% eamt12.tex)
%
% Contact: eamt2015@dlsi.ua.es

%%% To ease future customizations, various replaceables have been paramaterized
%%% as listed in the newcommands section

\documentclass[11pt]{article}
\usepackage{eamt15}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
%%% YOUR PACKAGES BELOW THIS LINE %%%

\usepackage{multirow}
\usepackage{natbib}
\usepackage{color}
\usepackage[small,bf]{caption}
\usepackage{graphicx}


\newcommand{\confname}{EAMT 2015}
\newcommand{\website}{http://www.eamt2015.org/}
\newcommand{\contactname}{the conference chairs (Felipe
  S\'anchez-Mart\'inez, Gema Ram\'irez-S\'anchez and Fred Hollowood)}
\newcommand{\contactemail}{eamt2015@dlsi.ua.es}
\newcommand{\conffilename}{eamt15}
\newcommand{\downloadsite}{http://www.eamt2015.org/}
\newcommand{\paperlength}{$8$ (eight)}
\newcommand{\shortpaperlength}{$4$ (four)}

\newcommand{\comment}[1]{\marginpar{\scriptsize\sf \textcolor{blue}{#1}}}


\title{Evaluating machine translation for assimilation via a gap-filling task}

\author{Ekaterina Ageeva\\
  School of Linguistics\\
  Higher School of Economics\\
  Moscow, Russia\\
  {\tt evageeva\_2@edu.hse.ru}\\[2ex]
  \textbf{Francis M. Tyers}\\
  HSL-fakultetet\\ 
  UiT Norgga\'{a}rktala\v{s} universitehta \\
  9017 Romsa, Norway \\
  {\tt francis.tyers@uit.no}
  \And
  Mikel L. Forcada\\
  Dept. Llenguatges i Sistemes Inform\`{a}tics\\
  Universitat d'Alacant, Spain \\
  {\tt mlf@dlsi.ua.es}  \\[2ex]
  \textbf{Juan Antonio P\'{e}rez-Ortiz} \\
  Dept. Llenguatges i Sistemes Inform\`{a}tics\\
  Universitat d'Alacant, Spain \\
  {\tt japerez@dlsi.ua.es}
}

\date{}

\begin{document}
\maketitle
\comment{EA: has not it first been used in trosterud12?}
\begin{abstract}
This paper provides additional observations on the viability of a strategy proposed in 2012 for evaluation of machine translation for assimilation purposes. The strategy involves human evaluators, who are asked to restore key words (fill gaps) in sentences. The evaluation method is applied to two translation directions. To reduce the amount of time required to prepare tasks and analyze results, an open-source task management system is introduced. The evaluation results suggest that the gap-filling task reflects users' understanding of text, and may be used to measure MT quality for assimilation purposes.
\end{abstract}

\section{Introduction}

As suggested by \citet{church93}, modern machine translation (MT) systems may
be divided into two broad categories according to their purpose: post-editing and assimilation systems. The output of the former is intended to be transformed into text comparable to human translation; the latter systems' goal is to enhance user's comprehension of text. Both kinds may be evaluated, either to control for quality in the development process or to compare the systems. Importantly, according to \citet{church93}, the evaluation methods must closely consider the system's primary purpose.

\comment{EA: I'd like a citation for the ``most massive application'', could you suggest something? It is from Mikel's paper, but no reference there -- should I cite Mikel?}
Despite the fact that assimilation (or gisting) is currently the most frequent application of MT, few methodologies are established for assimilation evaluation of MT. The methods include post-editing and comparison by bilingual experts \citep{ginesti09}, and multiple choice tests \citep{jones07,trosterud12}. These approaches are often costly and prone to subjectivity, see the discussion in \cite{oregan13}. As an alternative, the modification of Cloze \citep{taylor53} test was introduced for assimilation evaluation, first by \citep{trosterud12} as a supplementary, and then by \citep{oregan13} as a stand-alone method. Prior to this, Cloze tests have been used to evaluate raw MT quality \citep{vanslype79}, \citep{somers00}. As a principal difference, \citet{trosterud12} and \citet{oregan13} propose to fill gaps in the reference (human) translation. The gap-filling task models how well users comprehend the key points of the text. Thus, the method does not directly evaluate the quality of machine-produced text, but rather its usefulness in understanding the meaning of the original text. 

\comment{EA: I understand we selected tat-rus and eng-kaz because they were in development. why did we select eu-es?}
The gap-filling method has been successfully used to evaluate the Basque-English Apertium language pair. In this work we extend the evaluation to two more language pairs: Basque-Spanish and Tatar-Russian. For the latter pair, the evaluation served as a quality check in the period of active development during the Google Summer of Code'14 programme. In addition to evaluating, we explore new aspects of the experiment: the correlation between evaluators' scores, and the influence of the texts' linguistic domain. To facilitate the evaluation, we introduce an automated system which creates task sets from parallel corpora given a range of parameters (number of gaps in a sentence, hint type, gap filler, etc.), checks evaluators' answers, and calculates and formats generalized results. This system is integrated into the Appraise MT evaluation platform \citep{federmann12}; the code is open source and is available on Github\footnote{}.

We anticipate that the assessed MT systems will contribute to the users' understanding of text, that is, the evaluation will show better results in MT-assisted task type. We also expect to see different results depending on such parameters as text domain and the number of gaps in a sentence in relation to sentence length.

The paper is organised as follows: in section~\ref{sec:methodology} we describe the gap-filling method for assimilation evaluation: the task layout, the choice of words, and how the tasks are generated. Section~\ref{sec:setup} introduces the experimental material, the evaluators, the distribution of tasks and the evaluation procedure. In section~\ref{sec:results} we describe and discuss the experiment results. Finally, section~\ref{sec:conclusion} draws some conclusions.


\section{Methodology}
\label{sec:methodology}

\begin{table*}
  \begin{tabular}{|l|l|}
     \hline
     \textbf{Ref}   & Ayudas econ\'{o}micas para el tratamiento de toxicoman\'{i}­as en comunidades terap\'{e}uticas no concertadas. \\
     \textbf{Task}   & Ayudas econ\'{o}micas para el \{ \} de toxicoman\'{i}­as en comunidades terap\'{e}uticas no concertadas. \\
     \textbf{Src} & Komunitate terapeutiko itundu gabeetan toxikomaniak tratatzeko diru-laguntzak ematea. \\
     \textbf{MT}     & Comunidad terap\'{e}utico pactar gabeetan toxikomaniak las-ayudas de dinero para tratar dar. \\
     \hline
  \end{tabular}
  \caption{An example group of sentences showing the gapped sentence and hint types. Reference, MT and task sentences are in Spanish, the source sentence is in Basque.} 
  \label{table:example}
\end{table*}

\comment{EA: Francis, when I save cyrillic in article body, it gets replaced by ??. I committed a separate file with the table and the footnote that has the text.}

%\begin{table*}
%  \begin{tabular}{|l|l|}
%     \hline
%     \textbf{10\%}   & ???????? ???????; ??? ????? { } ????? 7 ?????????, ????? ?????? ??? ????? 100 ??????. \\
%     \textbf{20\%}   & ???????? ???????; ??? ????? { } ????? 7 ?????????, ????? { } ??? ????? 100 { }. \\
%     \textbf{30\%} & ???????? ???????; ??? ????? { } ????? 7 { }, ????? ?????? { } ????? 100 { }. \\
%     \hline
%  \end{tabular}
%  \caption{Example of different gap percentage settings.} 
%  \label{table:percentage}
%\end{table*}


This section discusses the reasoning behind the gap-filling method and task structure. The gap-filling method 
of evaluating machine translation for assimilation purposes is based on the following hypothesis: a reader's understanding 
of a given text correlates with the number of words they are able to correctly restore in the text. Therefore, the 
base of an assimilation task is a (reference) sentence, where some of the words are blacked out, or removed. The sentence \comment{EA: I'm sorry, I'm not sure what else I could explain here...}
is produced by a human (as opposed to machine-translated), and it is in the language known to evaluators, which is also 
the \emph{target language} of the machine translation system.
The additional elements of the task are what we call hints, or extra sentences that help the participant to understand 
the main sentence. There are two types of hints: first, the \emph{source}, which is semantically identical to \emph{reference}, 
also human-produced, but in the source language of the pair. The second type is the \emph{machine-translated} hint, which 
comes from the machine translation of the \emph{source} sentence. Table~\ref{table:example} shows a sample task, and Figure~\ref{figure:screenshot} shows the task in the online evaluation environment.\comment{EA: for some reason, the reference to screenshot is figure 2, while in the caption it says figure 1 (correctly)}

\begin{figure*}
  \centering
\includegraphics[width=0.9\textwidth,resolution=144]{appraise-scr}
\label{figure:screenshot}
 \caption{An example set of sentences in an online environment.}
\end{figure*}

\comment{EA: I don't really have a good explanation why we used source+mt tasks, except for being exhaustive. could you suggest anything?}
In the course of the experiment, the following hint combinations are offered:
\begin{description}
\item[Reference sentence only:] The participants are asked to fill the gaps without being given
any context. This task serves as a baseline score and as an indicator of gaps
that can be completed using common knowledge or language intuition (e.g.
idioms and strong collocations);
\item[Reference sentence and source sentence:] By setup, the
participants have no command of the source language, however, it may help them to fill
in proper nouns or loan words;
\item[Reference sentence and MT hint:] In addition to the task, the
participants see the source sentence translated via the MT system Apertium \citep{forcada11}. This type of task is
used for measuring the contribution of machine translation to understanding the
gist of text;
\item[Reference sentence and both hints:] This task is
added for completing the possible combination of task types, and to check
for unexpected insights, should they arise.
\end{description}

In order to prepare the evaluation questions, we determine and remove the keywords in the 
reference sentences. We consider two parameters: the list of allowed parts of speech (POS), and the 
number of gaps relative to sentence length (``gap density''). For the evaluations described in 
this paper we use gap densities of 10, 20 and 30 percent, and the following parts of speech: noun, 
proper noun, adjective, adverb and lexical verb (as opposed to auxiliary verb).

For each sentence, the list of candidate keywords is prepared. It is composed of all the words that 
fall into the allowed POS list. The number of gaps in the sentence is calculated based on 
sentence length and specified gap density. All reference sentences are over 10 words in length. Finally, the required number of keywords is selected 
from the candidate list in such a manner that the gaps are distributed evenly throughout the sentence. We start at a random word in a sentence and check whether it is a keyword. If yes, we remove it, and move \emph{n} words forward, going back to the beginning of sentence if necessary. The step length \emph{n} is the sentence length divided by the desired number of gaps. If the word is not a keyword, or has already been removed, we look at the next word instead. The process is repeated until the designated number of words has been removed, or until there are no more words in the keyword list.

Keyword removal is one of the most time-consuming steps in task preparation. It normally requires human effort, because we would like to determine the words that contribute the most to understanding the text as opposed to removing random words. In our setup, the above procedure is performed by a script integrated into the task generation pipeline. Parts of speech are determined with Apertium morphological analyser. To control for homonymy, we only allow the word into the candidate list if all of its possible part of speech attributions are on the POS list. For example, if we only allow nouns on the word list, and the word "fly" receives two possible part of speech attributions from the tagger, noun and verb, it is not considered for the candidate list.

Having prepared the sentence sets, we assemble them into XML formatted for the Appraise platform.

\section{Experimental set-up}
\label{sec:setup}

In this section we will discuss the evaluators, the evaluation procedure, and the tasks in more detail.

For each experiment we called for native speakers of target language of the language pair (i.e.
Spanish and Russian) who had no command of source language of the pair (Basque
and Tatar, respectively). 11 evaluators participated in the in Basque-Spanish experiment, and 28 in Tatar-Russian (although not everyone completed the task
in full, see discussion).

\comment{EA: maybe I should put this into the previous section?}
By design, our gap-filling tasks require a human translation (reference) of source sentences. Calling for a human translator, however, would significantly increase the resources needed for evaluation. We therefore use parallel text sources, which provide the same sentence in two languages simultaneously:
\begin{enumerate}
\item  For Basque-Spanish, from the corpus of legal texts ``Memorias de traducci\'on del
Servicio Oficial de Traductores del IVAP''\footnote{http://opendata.euskadi.net/memorias-de-traduccion-del-servicio-oficial-de-traductores-del-ivap/w79-contdata/es/};
\item  For Tatar-Russian, from the following sources on three different topics:
  \begin{enumerate}
\comment{EA: whoops, what is the best way to make it understand cyrillic?}
    \item  Casual conversations, from a textbook
    %\footnote{???????? ?.?. ? ??????? ???????? ??-????????. ??????: ????????? ??. ???-??, 1994. ? 320 ?. ISBN 5?298?00463?6 (???. 219, 220, 232, 233, 234)} of spoken Tatar;
    \item  Legal texts, from the Constitution and laws\footnote{http://tatarstan.ru} of Tatarstan;
    \item  News, from the President of Tatarstan website\footnote{http://president.tatarstan.ru/}.
  \end{enumerate}
\end{enumerate}

Each set features 36 pairs of sentences. For the Basque-Spanish experiment the pairs were drawn
randomly from the corpora, for Tatar-Russian, compiled by hand by the developer of the
language pair in Apertium.

\subsection{Procedure}

The evaluations took place online, in a system called Appraise \citep{federmann12}, which is 
designed specifically for various MT evaluation tasks. We adapted the code to 
accommodate for the gap-filling tasks. The tasks were uploaded into the system and 
manually distributed between the participants by the following rules:
\begin{enumerate}
\item  Each participant evaluates every sentence (understood as a succession of words),
a total of 36;
\item  Each participant evaluates 9 sentences in each of the four modes (see section~\ref{sec:methodology});
\item  All sentences of the set are evaluated with 10, 20 and 30\% of words removed;
\item Each participant may encounter a given sentence in only one of the percentage variations;
\item  Each sentence-mode-percentage combination is evaluated by more than one participant.
\end{enumerate}

The participants are given the instructions in their native language; these instructions are repeated above each task in the evaluation system. For the participants' convenience, the body of questions is split into smaller groups which allow indefinite breaks between them. The instructions are the 
following: read all the available hints and fill each gap with one suitable word, guessing if unsure.
Participants' answers are recorded and marked correct or incorrect automatically. In
addition, the time taken to fill the gaps in one sentence is recorded.

This variety of the gap-filling task requires open answers, and it is therefore possible that the participants may
provide words that fit the gaps well, but do not match the original answer. To account for
these cases, we process all the answers to detect possible synonyms (a method suggested by \citet{oregan13}. An answer is
considered a candidate synonym if it is given by two or more evaluators, and it does not match
the answer word. We record each candidate synonym along with the answer key and the
context sentence (see Table~\ref{table:syn} for an example). Based on this data, a native speaker of the target language decides
whether the candidate synonym is an acceptable replacement to the answer key in the given
context. We then check participants' results against the compiled synonym list and
increase scores where appropriate. On average, the scores improved by three percent in all evaluation modes. Candidate synonyms are extracted automatically from the evaluators' responses,  and each individual score is automatically updated according to the synonym list.

\begin{table*}
  \centering
  \begin{tabular}{|l|l|}
     \hline
     \textbf{Sentence:}   & Aprender a jugar y divertirse en el agua sin asumir riesgos. \\
     \textbf{Key:}   & asumir \\
     \textbf{Synonym:} & correr \\
     \hline
  \end{tabular}
  \caption{An example candidate synonym.} 
  \label{table:syn}
\end{table*}
\comment{EA: added}
The synonym lists for Basque-Spanish and Tatar-Russian contain 52 and 25 words, respectively. Time taken to compile one list depends on the number of candidate synonyms, and in our case was approximately 30 minutes. 
\comment{FMT: How long did making the synonym list take, how many synonyms were added? More details here. Also perhaps an example?}

\section{Results and discussion}
\label{sec:results}

The results are presented in this section. Tables \ref{table:res-eus-spa} and \ref{table:res-tat-rus} show
the proportion and standard deviation of correct answers depending on evaluation mode and gap density.\comment{FMT: We should probably integrate the other tables, and refer to them using labels.} The evaluators' correct answer percentage is averaged over the number of evaluators. In addition to the percentage of correct answers we kept a record of time taken to fill the gaps in one sentence. To reduce the noise from participants who were distracted during evaluation, when
calculating times we remove all the results over 6 minutes (the mode is approximately
two minutes). Typical time taken to complete one question varies from under one minute for tasks without hints and few gaps, to approximately two minutes for tasks with more hints and gaps.

% expected results
We expect to see the following trend: scores received in different task modes inside one gap density are observed, from highest to lowest, in tasks with MT and source hint, then MT hint only, then source hint only, and finally, no hint. We also expect that with the increase in gap density, time taken to fill the gaps also increases, and percentage of correct answers decreases. 

% a few notes on time
The latter trend holds: the average time taken to fill the gaps increases and the average percentage of correct answers decreases as the relative number of gaps goes up. The larger number of gaps in the sentence makes it more
difficult to predict the answer based on the context, and also leaves more room for
translation mistakes. Exploring different percentage-mode combinations, we may note that the 10\% no-hint tasks take the least time to complete. We would have expected longer completion time, since the participant must come up with their own answer unassisted. However, in the no-hint task the participant is required to read only one (reference) sentence, as opposed to two or three (reference and hints) in other tasks. Also, the number of gaps in 10\%-gap tasks is low, as it never exceeds three. We found that, as opposed to trying to devise the best word for no-hint gaps, the participants often resorted to filing these gaps with random words, which takes little time.
\comment{EA: anything else I could write about times?}

% this section talks about percentages in eu-es and tat-rus, and how everything is nice.
We will now discuss the percentage of correct answers based on task type. In general, tasks with MT hints score higher than tasks without MT hints. This aligns well with our expectations and suggests than machine translation helps to understand the provided text. In addition, tasks with source hints are completed better than tasks without hints, and the same relation holds between MT+source and MT-only types of tasks. 

Two records break this trend: the no-hint 10\% sentences in Basque--Spanish, which scored significantly higher than the source-hint in the same category, and MT+source 10\% sentences in Tatar--Russian, which we would have expected to score higher than the corresponding MT task. In the first case, this is largely due to the use of synonyms list. Before taking synonyms into account, the scores were 32 and 35 percent for source and no-hint tasks, respectively. This still shows a small difference in favor of no-hint tasks. However, the latter percentage increases significantly after we extend the answer list with synonyms. Such an increase suggests that, in this case, the content words were restored by 
\comment{EA: to JA: I am not sure if this qualifies as common sense. What I meant is that people correctly guessed the meaning of the word in place, and expressed it in their own manner. Changed it to semantic context, is this a better rephrasing?}
semantic context rather than through strong collocation. The second pattern, low scores in Tatar--Russian 10\% MT+source, does not stem from the task content. Instead, it is the result of the fixed order of tasks: the participants have always been given MT+source 10\% sentences first, followed by other task types. The participants have not received any training tasks before the main evaluations. Therefore, it is possible that the accommodation period is responsible for lower-than-expected scores in this mode of evaluation. 

It remains questionable whether we can compare results for different gap densities. The 10, 20 and 30\% sets were comprised
of the same sentences. However, in each case different words were removed. It appears
that some content words are easier to fill than the others. This may explain why in
Basque-Spanish the 20\% MT tasks are completed with better accuracy than 10\% tasks.

% participants and their suffering
It is worth noting that many participants reported feeling frustrated in
the course of evaluations, especially while working on the no-hint tasks. 6 out of 49
participants quit the experiment before completing it. Considering the importance of receiving the full set of evaluations, we must address the issue of participant motivation in the upcoming experiments.


\subsection{Annotator agreement}
After obtaining the results we calculated Krippendorff's alpha~\citep{krippendorff70} measure to represent annotator agreement, shown in Tables~\ref{table:alpha-eus-spa} and~\ref{table:alpha-tat-rus}. We selected this measure because of its compatibility with more than two annotators per task and the missing data (not all the gaps were evaluated). To calculate Krippendorff's alpha we used an algorithm implementation by Thomas Grill\footnote{http://grrrr.org}, dividing the answers in each gap into two categories: correct and incorrect. The previously obtained synonym lists were taken into account, i.e. if the two answers are different but both correct, they fall into one category. The measure was calculated separately for each hint and percentage combination.

As the MT and MT+source hints are introduced, the agreement between annotators increases (measures closer to 1): the annotators are more consistently correct or incorrect in each given sentence.\comment{EA: I cite Juan Antonio here, because I am not completely sure what alpha shows about our experiment (lack of knowledge on my part)}  The agreement measure for the same sentences without hints is closer to zero, which contributes to the reliability of our methodology. We note the outlier score in Tatar--Russian 10\% source tasks, which has the most contribution from the news texts (agreement of 0.917). This set of sentences contains many loan words, which have similar form in Tatar and Russian (e.g. president, minister, championship), and are understood by Russian speakers. The gaps with loan words have mostly been filled correctly, while there was some disagreement in other gaps.


\begin{table*}
\centering
  \begin{tabular}{|l|r|r|r|r|}
  \hline
 \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
\hline
10\%&0.496&0.517&0.400&0.124\\
20\%&0.714&0.700&0.358&0.275\\
30\%&0.559&0.430&0.406&0.300\\

\hline
\end{tabular}
\caption {Tatar--Russian: Krippendorff Alpha measure} 
\label{table:alpha-eus-spa}
\end{table*}

\begin{table*}
\centering
  \begin{tabular}{|l|r|r|r|r|}
  \hline
 \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
\hline
10\%&0.598&0.459 &0.711 &0.517 \\
20\%&0.740&0.667 &0.473 &0.261 \\
30\%&0.534&0.581 &0.411 &0.412 \\
\hline
\end{tabular}
\caption {Tatar--Russian: Krippendorff Alpha measure} 
\label{table:alpha-tat-rus}
\end{table*}

% a little on categories
\subsection{Results for different domains}
\comment{EA: and what would we like to say about those? The results depend heavily on the Tatar-Russian system. Do we have to provide info on what vocabulary it handles better, what it has and has not been trained on? Or do we just say that casual texts are easier to fill than legal texts?}
For Tatar--Russian language pair the participants were offered texts from three different domains (in equal proportions): casual conversations, legal texts and news. The results by domains are displayed in tables \ref{table:rus-casual}, \ref{table:rus-legal}, and \ref{table:rus-news}. The MT system used in the evaluation has been targeted to translate texts from all three of the domains. Taking into consideration the above discussion of 10\% MT+Source tasks, we observe similar results across the three categories. Note that, in casual and news texts, the source sentences paired with MT significantly improve participants' performance compared to MT-only task mode. This may be due to the fact that many words are borrowed from Russian into Tatar, and are in fact understood by Russian speakers.

% results with synonyms

\begin{table*}
\centering
  \begin{tabular}{|l|r|r|r|r|}
  \hline
 \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
\hline
10\%&62.19 +/- 32.07&58.04 +/- 27.96&39.90 +/- 38.96&48.79 +/- 39.84\\
20\%&64.81 +/- 30.03&70.18 +/- 26.98&31.37 +/- 28.42&30.91 +/- 30.22\\
30\%&48.13 +/- 26.08&39.55 +/- 23.90&25.94 +/- 19.67&18.07 +/- 18.15\\
\hline
\end{tabular}
\caption {Basque--Spanish: Average number of gaps successfully filled (\%), using a synonym list} \label{table:res-eus-spa}
\end{table*}

\begin{table*}
\centering
  \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
\hline
10\%&63.76 +/- 41.15&57.02 +/- 41.57&53.52 +/- 43.57&45.96 +/- 40.86\\
20\%&59.77 +/- 33.10&64.93 +/- 30.95&45.60 +/- 31.23&38.56 +/- 32.11\\
30\%&55.83 +/- 26.13&58.77 +/- 28.07&40.00 +/- 28.16&35.45 +/- 29.76\\
    \hline
  \end{tabular}
  \caption{Tatar--Russian: Average number of gaps successfully filled (\%), using a synonym list}
  \label{table:res-tat-rus}
\end{table*}

% results by text mode in tat-rus, with synonyms

\begin{table*}

\centering
\begin{tabular}{|r |*{4}{c}|}
\hline
  &\multicolumn{4}{c|}{\textbf{Evaluation mode}}\\
\hline
\textbf{Gap percentage} & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
10\%&63.64 +/- 44.84&63.64 +/- 43.12&62.12 +/- 46.11&53.03 +/- 44.28\\
20\%&73.33 +/- 31.51&63.33 +/- 35.85&41.11 +/- 27.88&37.78 +/- 30.94\\
30\%&70.28 +/- 31.12&59.58 +/- 23.89&39.44 +/- 27.10&37.71 +/- 33.32\\
\hline
\end{tabular}
\caption {Tatar--Russian, casual conversations: Average number of gaps successfully filled (\%), using a synonym list} \label{table:rus-casual} 
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|r |*{4}{c}|}
\hline
  &\multicolumn{4}{c|}{\textbf{Evaluation mode}}\\
\hline
\textbf{Gap percentage} & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
10\%&53.02 +/- 40.31&68.21 +/- 34.99&38.77 +/- 37.82&32.95 +/- 34.88\\
20\%&60.57 +/- 24.98&65.76 +/- 24.16&50.28 +/- 33.55&48.31 +/- 33.52\\
30\%&50.06 +/- 26.25&48.30 +/- 28.87&40.05 +/- 27.37&34.15 +/- 28.73\\
\hline
\end{tabular}
\caption {Tatar--Russian, legal texts: Average number of gaps successfully filled (\%), using a synonym list} \label{table:rus-legal} 
\end{table*}

\begin{table*}
\centering
\begin{tabular}{|r |*{4}{c}|}
\hline
  &\multicolumn{4}{c|}{\textbf{Evaluation mode}}\\
\hline
\textbf{Gap percentage} & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
10\%&53.33 +/- 37.86&59.77 +/- 43.66&57.47 +/- 42.12&49.44 +/- 39.08\\
20\%&59.33 +/- 33.89&48.56 +/- 35.40&46.53 +/- 31.99&28.97 +/- 28.79\\
30\%&58.14 +/- 22.42&61.49 +/- 22.11&40.51 +/- 30.10&34.81 +/- 27.01\\
\hline
\end{tabular}
\caption {Tatar--Russian, news: Average number of gaps successfully filled (\%), using a synonym list} \label{table:rus-news} 
\end{table*}

\section{Conclusions}
\label{sec:conclusion}
We have conducted assimilation evaluation of two Apertium translation directions: Basque--Spanish and Tatar--Russian. The results suggest that this evaluation method reflects the contribution of MT to users' understanding of text. The version of the toolkit used in this 
\comment{EA to JA: by our repository I mean my github, is that ok?}
experiment may be downloaded from our repository\footnote{}.

In addition to the mentioned language pairs, some preliminary experiments were carried out for the English--Kazakh pair. It remains questionable whether we may draw reliable conclusions from this part of the experiment. By design, the evaluation tasks are distributed evenly among the participants, in such a manner that each sentence is evaluated in all modes or gap densities by several people. The English--Kazakh evaluations have been set up for 11 participants, however, only 8 participants attempted their tasks, and 6 of them completed it in full. As a result, the evaluation became unbalanced: out of 4 sets of sentences, three have been evaluated by three, two and one participant, respectively, and the fourth set has been partially completed by only one participant. A total of 40 (about 10\%) gaps have not been filled by any of the participants. Therefore, we do not report the English--Kazakh evaluation results here.

The experiment may be repeated for any language pair (provided a parallel corpus) and
any machine translation system. Based on our experience, we would like to suggest the following amendments to the procedure:
\begin{enumerate}
\item If the experiment is conducted with different gap densities, each consecutive set of sentences should be derived from the set with lower gap density. To put another way, the sentence with e.g. 20\% gaps should contain all the gaps of the same sentence in 10\% mode, plus any additional gaps;
\item Unless the evaluation is targeted at a specific text domain, it may be beneficial to include a stylistic variety of texts in the initial corpus. Neighboring sentences on the same topic may assist the users in gap-filling tasks;
\item If possible, increase the number of evaluators, or reduce the number of questions per participant. In the above experiments each participant filled from 110 to 187 gaps, divided into small groups. Reducing the amount of work may increase task completion rate;
\item To account for the adaptation period, provide training tasks before the main evaluations take place.
\end{enumerate}

\section{Acknowledgments}

This work has been partly funded by the Spanish Ministerio de Econom{\'i}a y Competitividad through project TIN2012-32615 and by the Google Summer of Code programme. We would like to thank the volunteers who participated in the evaluations.

\bibliographystyle{apalike}

\bibliography{2015-eamt-assim}

\end{document}
