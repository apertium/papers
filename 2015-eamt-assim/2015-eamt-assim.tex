%
% File eamt15.tex, an adaptation of eamt14.tex (which was a copy of
% eamt12.tex)
%
% Contact: eamt2015@dlsi.ua.es

%%% To ease future customizations, various replaceables have been paramaterized
%%% as listed in the newcommands section

\documentclass[11pt]{article}
\usepackage{eamt15}
\usepackage{times}
\usepackage{latexsym}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
%%% YOUR PACKAGES BELOW THIS LINE %%%

\usepackage{multirow}
\usepackage{natbib}
\usepackage{color}

\newcommand{\confname}{EAMT 2015}
\newcommand{\website}{http://www.eamt2015.org/}
\newcommand{\contactname}{the conference chairs (Felipe
  S\'anchez-Mart\'inez, Gema Ram\'irez-S\'anchez and Fred Hollowood)}
\newcommand{\contactemail}{eamt2015@dlsi.ua.es}
\newcommand{\conffilename}{eamt15}
\newcommand{\downloadsite}{http://www.eamt2015.org/}
\newcommand{\paperlength}{$8$ (eight)}
\newcommand{\shortpaperlength}{$4$ (four)}

\newcommand{\comment}[1]{\marginpar{\scriptsize\sf \textcolor{blue}{#1}}}


\title{Evaluating machine translation for assimilation}

\author{Ekaterina Ageeva\\
  School of Linguistics\\
  Higher School of Economics\\
  Moscow, Russia\\
  {\tt evageeva\_2@edu.hse.ru}\\[2ex]
  \textbf{Francis M. Tyers}\\
  HSL-fakultetet\\ 
  UiT Norgga\'{a}rktala\v{s} universitehta \\
  9017 Romsa, Norway \\
  {\tt francis.tyers@uit.no}
  \And
  Mikel L. Forcada\\
  Dept. Llenguatges i Sistemes Inform\`{a}tics\\
  Universitat d'Alacant, Spain \\
  {\tt mlf@dlsi.ua.es}  \\[2ex]
  \textbf{Juan Antonio P\'{e}rez-Ortiz} \\
  Dept. Llenguatges i Sistemes Inform\`{a}tics\\
  Universitat d'Alacant, Spain \\
  {\tt japerez@dlsi.ua.es}
}

\date{}

\begin{document}
\maketitle
\comment{EA: has not it first been used in trosterud12?}
\begin{abstract}
This paper provides additional observations on the viability of a strategy proposed in 2012 for evaluation of machine translation for assimilation purposes. The evaluation method is applied to three translation directions. To reduce preparation time, an automatic open-source task management system is introduced.
\comment{FMT: Abstract should mirror/summarise the article, e.g. include something about conclusions too}
\comment{EA: will add conclusions after writing the section on them}
\end{abstract}

\comment{EA: I noticed some spelling edits, e.g. organize -> -se, program -> -mme. Is it important that we use British spelling? If so, may I count on someone to go through the paper and correct it? I am not sure I can use British spelling consistently.}

\section{Introduction}

As suggested by \citet{church93}, modern machine translation (MT) systems may
be divided into two broad categories according to their purpose: post-editing systems and assimilation systems. Both kinds may be systematically evaluated, either to control for quality in the development process or to compare the systems. Importantly, according to \citet{church93}, the evaluation methods must closely consider the system's primary purpose.
\comment{FMT: Apertium can also be used for postedition, e.g. spanish-catalan, and some Google translate directions aren't suitable for that. Perhaps reword or remove this}
\comment{EA: removed}

\comment{EA: I'd like a citation for the ``most massive application'', could you suggest something?}
Despite the fact that assimilation (or gisting) is currently the most massive application of MT, few methodologies are established for assimilation evaluation of MT. The methods include post-editing and comparison by bilingual experts \citep{ginesti09}, and multiple choice tests \citep{jones07,trosterud12} These approaches are often costly and prone to subjectivity, see the discussion in \cite{oregan13}. As an alternative, the modification of Cloze test was introduced for assimilation evaluation, first in \citep{trosterud12} as a supplementary, and then in \citep{oregan13} as a stand-alone method. Prior to this, Cloze tests have been used to evaluate raw MT quality, see e.g. \citep{vanslype79}, \citep{somers00}. As a principal difference, \citet{trosterud12} and \citet{oregan13} propose to fill the gaps in the reference (human) translation.

The gap-filling method has been successfully used to evaluate the Basque-English Apertium language pair. In this work we extend the evaluation to three more language pairs: Basque-Spanish, English-Kazakh and Tatar-Russian. For the latter two pairs, the evaluation served as a quality check in the period of active development during the Google Summer of Code'14 programme. In addition to evaluating, we explore new aspects of the experiment: the correlation between evaluators' scores, and the influence of the texts' linguistic domain. To facilitate the evaluation, we introduce an automated task creation system. This system is integrated into the Appraise MT evaluation platform \citep{federmann12}; the code is open source and is available on Github\footnote{https://github.com/Sereni/Appraise}.

We anticipate that MT systems under investigation will contribute to the users' understanding of text. We also expect to see different results depending on text domain and on the number of words left out from the task.

The paper is organised as follows: in section~\ref{sec:methodology} we describe the gap-filling method for assimilation evaluation: the task layout, the choice of words, and how the tasks are generated. Section~\ref{sec:setup} introduces the experimental material, the evaluators, the distribution of tasks and the evaluation procedure. In section~\ref{sec:results} we describe and discuss the experiment results. Finally, section~\ref{sec:conclusion} draws conclusions.


\section{Methodology}
\label{sec:methodology}
\comment{FMT: This could benefit from an example i think, something like table1? }

\begin{table*}
  \begin{tabular}{|l|l|}
     \hline
     \textbf{Ref}   & Ayudas econ\'{o}micas para el tratamiento de toxicoman\'{i}­as en comunidades terap\'{e}uticas no concertadas. \\
     \textbf{Task}   & Ayudas econ\'{o}micas para el \{ \} de toxicoman\'{i}­as en comunidades terap\'{e}uticas no concertadas. \\
     \textbf{Src} & Komunitate terapeutiko itundu gabeetan toxikomaniak tratatzeko diru-laguntzak ematea. \\
     \textbf{MT}     & Comunidad terap\'{e}utico pactar gabeetan toxikomaniak las-ayudas de dinero para tratar dar. \\
     \hline
  \end{tabular}
  \caption{An example of a set of sentences.} 
  \label{table:example}
\end{table*}

This section discusses the reasoning behind the gap-filling method and task structure. The gap-filling method 
of evaluating machine translation for assimilation purposes is based on the following notion: a reader's understanding 
of a given text correlates with the number of keywords they are able to correctly restore in the text. Therefore, the 
base of an assimilation task is a (reference) sentence, where some of the keywords are blacked out, or removed. The sentence 
is produced by a human (as opposed to machine-translated), and it is in the language known to evaluators, which is also 
the \emph{target language} of the machine translation system.
\comment{FMT: It would also be nice to have a screenshot here}
The additional elements of the task are what we call hints, or extra sentences that help the participant to understand 
the main sentence. There are two types of hints: first, the \emph{source}, which is the same sentence as \emph{reference}, 
also human-produced, but in the source language of the pair. The second type is the \emph{machine-translated} hint, which 
comes from the machine translation of the \emph{source } sentence.

\comment{EA: I don't really have a good explanation why we used source+mt tasks, except for being exhaustive. could you suggest anything?}
In the course of the experiment, the following hint combinations are offered:
\begin{description}
\item[Reference sentence only:] The participants are asked to fill the gaps not being given
any context. This task serves as a baseline score and as an indicator of phrases
that can be completed using common knowledge or language intuition (e.g.
idioms and strong collocations);
\item[Reference sentence and source sentence:] By setup, the
participants have no command of the source language, however, it may help them to fill
in proper nouns or loan words;
\item[Reference sentence and MT hint:] In addition to the task, the
participants see the source sentence translated via Apertium. This type of task is
used for measuring the contribution of machine translation to understanding the
gist of text;
\item[Reference sentence and both hints:] This task is
added for completing the possible combination of task types, and to check
for unexpected insights, should they arise.
\end{description}

In order to prepare the evaluation questions, we determine and remove the keywords in the 
reference sentences. We consider two parameters: the list of allowed parts of speech (POS), and the 
number of gaps relative to sentence length (``gap density''). For the evaluations described in 
this paper we use gap densities of 10, 20 and 30 percent, and the following parts of speech: noun, 
proper noun, adjective, adverb and lexical verb (as opposed to auxiliary verb).

For each sentence, the list of candidate keywords is prepared. It constitutes all the words that 
fall into the allowed POS list. The number of gaps in the sentence is calculated based on 
sentence length and specified gap density. Finally, the required number of keywords is selected 
from the candidate list in such a manner that the gaps are distributed evenly throughout the sentence.

Keyword removal is one of the most time-consuming steps in task preparation. In our setup, the above procedure is performed by a script integrated into the task generation pipeline. Parts of speech are determined with Apertium morphological analyser. To control for homonymy, we only allow the word into the candidate list if all of its possible part of speech attributions are on the POS list. 

Having prepared the sentence sets, we assemble them into XML formatted for the Appraise platform.

%To exclude headlines and also to allow at least one gap in 10\%-gap sentences, only the
%sentences over 10 words long are used in the tasks.
%The sentences for each experiment are drawn from various parallel text sources:
%\begin{enumerate}
%\item  For Basque-Spanish, from the corpus of legal texts Memorias de traducci\'on del
%Servicio Oficial de Traductores del IVAP 3
%\item  For English-Kazakh, from the official website of the President of the Republic of
%Kazakhstan 4
%\item  For Tatar-Russian, from the following sources on three different topics:
%  \begin{enumerate}
%    \item  Casual conversations, from a textbook 5 of spoken Tatar;
%    \item  Legal texts, from the Constitution and laws 6 of Tatarstan;
%    \item  News, from the President of Tatarstan website 7 .
%  \end{enumerate}
%\end{enumerate}
%
%Each set features 36 pairs of sentences. For the first two experiments the pairs are drawn
%randomly from the corpora, for Tatar-Russian, compiled by hand by the developer of the
%language pair. After processing, the material is transformed into XML used for uploading
%into the evaluation system (see section 2.4).

\section{Experimental set-up}
\label{sec:setup}

In this section we will discuss the evaluators, the evaluation procedure, and the tasks in more detail.

For each experiment we called for native speakers of L2 of the language pair (i.e.
Spanish, Kazakh, and Russian) who had no command of L1 of the pair (Basque, English
and Tatar, respectively). 11 evaluators participated in the in Basque-Spanish experiment, 8 in
English-Kazakh, and 28 in Tatar-Russian (although not everyone has completed their task
in full, see discussion).

\comment{EA: maybe I should put this into the previous section?}
By design, our gap-filling tasks require a human translation (reference) of source sentences. Calling for a human translator, however, would significantly increase the resources needed for evaluation. We therefore use parallel text sources, which provide the same sentence in two languages simultaneously:
\begin{enumerate}
\item  For Basque-Spanish, from the corpus of legal texts Memorias de traducci\'on del
Servicio Oficial de Traductores del IVAP\footnote{http://opendata.euskadi.net/memorias-de-traduccion-del-servicio-oficial-de-traductores-del-ivap/w79-contdata/es/};
\item  For English-Kazakh, from the official website of the President of the Republic of
Kazakhstan\footnote{http://www.akorda.kz/};
\item  For Tatar-Russian, from the following sources on three different topics:
  \begin{enumerate}
\comment{EA: whoops, what is the best way to make it understand cyrillic?}
    \item  Casual conversations, from a textbook\footnote{FIXME, 1994. ? 320 ?. ISBN 5?298?00463?6 (???. 219, 220, 232, 233, 234)} of spoken Tatar;
    \item  Legal texts, from the Constitution and laws\footnote{http://tatarstan.ru} of Tatarstan;
    \item  News, from the President of Tatarstan website\footnote{http://president.tatarstan.ru/}.
  \end{enumerate}
\end{enumerate}

Each set features 36 pairs of sentences. For the first two experiments the pairs are drawn
randomly from the corpora, for Tatar-Russian, compiled by hand by the developer of the
language pair.

\subsection{Procedure}

The evaluations take place online, in a system called Appraise \citep{federmann12}, which is 
designed specifically for various MT evaluation tasks. We adapted the code to 
accommodate for the gap-filling tasks. The tasks are uploaded into the system and 
manually distributed between the participants by the following rules:
\begin{enumerate}
\item  Each participant evaluates every sentence (understood as a succession of words),
a total of 36;
\item  Each participant evaluates 9 sentences in each of the four modes (see section~\ref{sec:methodology});
\item  All sentences of the set are evaluated with 10, 20 and 30\% of words removed;
\item  Each sentence-mode-percentage combination is evaluated by at least three participants.
\end{enumerate}

The participants are given the instructions in their native language above each task. The
tasks are split into smaller sets for the participants' convenience. The instructions are the 
following: read all the available hints and fill each gap with one suitable word, guessing if unsure.
Participants' answers are recorded and marked correct or incorrect automatically. In
addition, the time taken to fill the gaps in one sentence is recorded.

This variety of the gap-filling task requires open answers, and it is therefore possible that the participants may
provide words that fit the gaps well, but do not match the original answer. To account for
these cases, we process all the answers to detect possible synonyms (a method suggested by \citet{oregan13}. An answer is
considered a candidate synonym if it is given by two or more evaluators, and it does not match
the answer key. We record each candidate synonym along with the answer key and the
context sentence. Based on this data, a native speaker of the target language decides
whether the candidate synonym is indeed synonymic to the answer key in the given
context. We then check participants' results against the compiled synonym list and
increase scores where appropriate. Candidate synonym extraction and score update is performed automatically.

\begin{table*}
  \begin{tabular}{|l|l|}
     \hline
     \textbf{Sentence:}   & Aprender a jugar y divertirse en el agua sin asumir riesgos. \\
     \textbf{Key:}   & asumir \\
     \textbf{Synonym:} & correr \\
     \hline
  \end{tabular}
  \caption{An example candidate synonym.} 
  \label{table:syn}
\end{table*}
\comment{EA: added}
The synonym lists for Basque-Spanish, English-Kazakh and Tatar-Russian contain 52, 38 and 25 words, respectively. Time taken to compile one list depends on the number of candidate synonyms, and in our case was approximately 30 minutes.
\comment{FMT: How long did making the synonym list take, how many synonyms were added? More details here. Also perhaps an example?}

\comment{EA: the sections below are from the report and have not been edited yet.}

\section{Expected results}

We expect that, for tasks with MT assistance, the users will fill more gaps correctly,
which would correspond to better understanding of text. Therefore, the percentage of
correct answers should be higher for MT and two-hint kinds of tasks than for tasks with
source sentence as a hint and no hint at all. If the percentage were lower, we would
expect an error in experiment design.

\section{Results}
\label{sec:results}

The experiment results are presented in the tables below. The first table of each set shows
the proportion of correct answers depending on evaluation mode and gap density.\comment{FMT: We should probably integrate the other tables, and refer to them using labels.} The
percentage is spread across all evaluators, that is, we divide the number of correct
answers given by all evaluators by the number of questions answered by all evaluators.
The second table of the set shows average time it took to fill the gaps in one sentence. To
reduce the noise from participants who were distracted during evaluation, when
calculating times we remove all the results over 6 minutes (the mode is approximately
two minutes).

\begin{table}
  \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
    10\%                                    &   58.02            & 55.70       & 35.06        & 45.45        \\
    20\%                                    &   65.96            & 68.42       & 33.85        & 33.56        \\
    30\%                                    &   49.84            & 37.28       & 24.23        & 18.93        \\
    \hline
  \end{tabular}
  \caption{Basque--Spanish: Gaps successfully filled (\%), using a synonym list}
  \label{table:res-eus-spa}
\end{table}

\begin{table}
  \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
    10\%                                    & 34.78              & 39.02       & 40.00        & 31.43        \\
    20\%                                    & 43.84              & 33.80       & 47.83        & 60.34        \\
    30\%                                    & 41.38              & 36.56       & 44.94        & 31.09        \\
    \hline
  \end{tabular}
  \caption{English--Kazakh: Gaps successfully filled (\%), using a synonym list}
  \label{table:res-eng-kaz}
\end{table}

\begin{table}
  \begin{tabular}{|l|r|r|r|r|}
    \hline
    \multirow{2}{*}{\textbf{Density}} & \multicolumn{4}{|c|}{Evaluation mode} \\\cline{2-5}
                                            & \textbf{MT \& Src} & \textbf{MT} & \textbf{Src} & \textbf{No hint} \\
    10\%                                    & 53.89              & 66.06       & 54.43        & 46.34        \\
    20\%                                    & 65.33              & 60.73       & 48.39        & 39.43        \\
    30\%                                    & 59.90              & 52.59       & 39.66        & 38.39        \\
    \hline
  \end{tabular}
  \caption{Tatar--Russian: Gaps successfully filled (\%), using a synonym list}
  \label{table:res-tat-rus}
\end{table}



\section{Conclusions}
\label{sec:conclusion}
The following trends may be derived from the data above:
\begin{enumerate}
\item  In general, tasks with MT assistance score higher than task without MT
assistance;
\item  The more words are removed from context, the more difficult it is to fill the gaps
(see below for exceptions);
\item  Task without hints take less time to complete than others.
\end{enumerate}
The first two trends align with our expectations. Higher scores on MT tasks indicate
better understanding of text. The large number of gaps in the sentence makes it more
difficult to predict the answer based on the context, and also leaves more room for
translation mistakes. We would expect that tasks without hints take the largest amount of
time rather than that the smallest, since the participant must come up with their own
answer unassisted. However, in the no-hint task the participant reads only one sentence,
as opposed to two or three in other tasks, which may be the source of difference.
In Basque-Spanish and Tatar-Russian these trends are defined reasonably well. In
English-Kazakh, however, the scores for source-hint task (a parallel sentence in English
for assistance) are systematically higher than for other types of tasks. In addition, a 20\%no-hint task stands out with 60\% accuracy, which is substantially higher than any other
scores for this experiment.
Having analysed the tasks and the responses for English-Kazakh, I have found the
following possible reasons of the obtained results:
\begin{enumerate}
\item  The experiment was planned for 11 evaluators, although only 8 attempted the
tasks and 6 completed them in full. Because of this, the sentences were evaluated
once or twice, and some were not attempted at all. To reliably compare results
across different categories, all tasks must be completed;
\item  Due to an error during experiment design, some sentences are not repeated across
categories, which also suggests that we may not compare these percentages;
\item  The outlier score of 60\% is made up by three sentences. Two of these scored
unusually high. It appeared that one of them is a news headline, and the other
contains clich\'es such as ``express condolences''. There may also be interference
from other tasks of the set, because some of the sentences are drawn from the
same news article. We would expect the same sentences to contribute to higher
percentage in other task modes, but they were not completed there;
\item  It may be the case that the participants have at least some command of English,
which would have helped them to derive extra information in the source-hint
\end{enumerate}
It remains questionable whether we can compare results for different gap densities. In the
Basque-Spanish and Tatar-Russian experiments the 10, 20 and 30\% sets were comprised
of the same sentences. However, in each case different words were removed. It appears
that some content words are easier to fill than the others. This may explain why in
Basque-Spanish the 20\% MT tasks are completed with better accuracy than 10\% tasks.
It may be worth noting that the majority of the participants reported feeling frustrated in
the course of their work, especially while working on the no-hint tasks. 9 out of 51
participants quit the experiment before completing it.

\section{Future work}
\label{sec:futurework}
The experiment may be repeated for any language pair (provided a parallel corpus) and
any machine translation system. To enhance the results, the procedure may be amended
in the following ways:
\begin{enumerate}
\item  Reduce the number of questions per participant. In the above experiments each
participant filled from 110 to 187 gaps, divided into small groups. Reducing the
amount of work may increase task completion rate;
\item  Increase the number of participants per experiment. The more evaluators work on
every question, the more reliable the data;
\item  Design the tasks with different gap densities in such a way that e.g. the 20\% task
contains all the gaps of the 10\% task, plus any additional ones;
\item  Draw sentences from large corpora. This will reduce the influence of previous
tasks, which may provide the context for other tasks in the set.
\end{enumerate}


\bibliographystyle{apalike}

\bibliography{2015-eamt-assim}

\end{document}
