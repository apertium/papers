%Try2: akshay minocha
%
% File coling2014.tex
%
% Contact: jwagner@computing.dcu.ie

\documentclass[11pt]{article}
\usepackage{coling2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage[small,bf]{caption}
\usepackage{multirow}


\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algcompatible}
\usepackage[noend]{algpseudocode}

%\setlength\titlebox{5cm}

\title{Subsegmental language detection in Celtic language text}

\author{}
\author{Akshay Minocha \\
  IIIT Hyderabad  \\
  Hyderabad (India) \\
  {\small {\tt akshay.minocha@students.iiit.ac.in}} \\\And
  Francis M. Tyers \\
  Giellatekno \\ %/CLEAR \\
  UiT Norgga \'arktala\v{s} universitehta  \\
  9017 Romsa (Norway) \\
  {\small {\tt francis.tyers@uit.no}} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper describes an experiment to perform language identification on a sub-sentence basis. The typical
  case of language identification is to detect the language of documents or sentences. However, it may be the 
  case that a single sentence or segment contains more than one language. This is especially the case in texts
  where \emph{code switching} occurs. 

%  Bilingual communities contribute majorly to the code-switching phenomena, where speakers produce sentences containing words and expressions from a second language. With the increase in the data on the web, specially on informal platforms like that of the social media, it becomes increasingly important to detect such irregularities and handle them appropriately. In this paper we propose ways of dealing with code-switching and detecting sub-segments for texts in three Celtic languages: Breton, Irish and Welsh. 
\end{abstract}

\section{Introduction}
\vspace{-0.132cm}
\label{intro}
Determining the language of a piece of text is one of the first steps that must be taken
before proceeding with further computational processing. This task has received a substantial amount of
attention in recent years \cite{cavnar1994n,lui2012langid}. However, previous research has on the whole assumed
that a given text will be in a single language. When dealing with text from formal domains,
this may be the case --- although there are exceptions --- such as quotations embedded in
the text in another language. But when dealing with informal text, particularly in languages
where the speech community is predominantly bi- or multi-lingual, this assumption may not hold.
%Such as
%code switching, quotations, named entities, interjections, translations --- either of the full text, or 
%part of the text, comments, etc. 

The work presented in this paper was motivated by the problems in normalising non-standard input
for the Celtic languages as a precursor to machine translation. When applying a normalisation 
strategy to a piece of text, it is necessary to first know the language of the piece of text you are applying it to.
%Table~\ref{table:examples} shows some examples of how code switching is present in text
%There are several ways in which an informal text may contain parts in different languages.
%Some of these are presented in Table~\ref{table:examples}.

The remainder of the paper is laid out as follows. In section~\ref{sec:method} we describe the problem
in more detail and look at relevant prior work before proposing a novel method of sub-sentential
language detection. Section~\ref{sec:eval} describes the evaluation methodology. Then in section~\ref{sec:results}
we present the results of our method and compare it against several other possible methods. Finally, section~\ref{sec:conclusions}
presents future work and conclusions.
\blfootnote{
    %
    % for review submission
    %
    %\hspace{-0.65cm}  % space normally used by the marker
    %Place licence statement here for the camera-ready version, see
    %Section~\ref{licence} of the instructions for preparing a
    %manuscript.
    %
     % final paper: en-uk version (to license, a licence)
    
     \hspace{-0.65cm}  % space normally used by the marker
     This work is licensed under a Creative Commons 
     Attribution 4.0 International Licence.
     Page numbers and proceedings footer are added by
     the organisers.
     Licence details:
     \url{http://creativecommons.org/licenses/by/4.0/}
     
    % % final paper: en-us version (to licence, a license)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licenced under a Creative Commons 
    % Attribution 4.0 International License.
    % Page numbers and proceedings footer are added by
    % the organizers.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
}


%\begin{table}[h]
%\begin{center}
%\vspace{-0.3cm}
%%\begin{tabular}{ll}
%\textbf{Code switching}: & You're a [Meirice\'{a}nach, c\'{e}n f\'{a}th] are you [foghlaim Gaeilge?!] \\
%\textbf{Quotations}: & The anthem starts with the words [`Mae hen wlad fy nhadau\ldots'] \\
%\textbf{Named entities}: & [Dr Jekyll] ha [Mr Hyde] embannet gant [\'{E}ditions Aber] \\
%\textbf{Interjections}: & Hey, that's great, [diolch yn fawr!] \\
%\textbf{Translations}: & Bloavezh mat d'an holl ! [Bonne ann\'{e} \`{a} tous !] \\
%\end{tabular}
%\caption{Some examples of text segments containing more than one language}
%\label{table:examples}
%\end{center}
%\end{table}
%\vspace{-0.65cm}

\section{Related Work}
\vspace{-0.132cm}
\label{sec:relwork}

Code-switching and segment detection problems have been the subject of previous research. A good deal of work
has been done on detecting code-switched segments in speech data \cite{chan2004detection,lyu2006language}.
It is seen that language modelling techniques have shown promise earlier, such as in \newcite{yu2013identification}, 
the experiment on Mandarin-Taiwanese sentences show a high accuracy in terms of detecting code-switched sentences. 

In \newcite{chan2004detection} the authors have made use of the bi-phone probabilities and calculated them to measure a confidence metric, 
to \cite{lyu2006language} which has made use of named syllable-based duration classification, 
which uses the tonal syllable properties along with the speech signals to help predict the code switch points. In \newcite{yeong2010language} 
 the authors use syllable structure information to identify words in code-switched text in Malay-English, however they did not 
recognise segments in running text, only identifying individual words.

%Our idea of going to character level inspection has been inspired both by the language modelling techniques used by the community as well as the acoustic modelling research. 

\section{Methodology}
\vspace{-0.132cm}
\label{sec:method}

As the number of possible languages for each segment is in theory the set of all the world's written languages, we take 
a decision to simplify the task by only looking at texts in the Celtic language and the corresponding majority language spoken
where the Celtic language is spoken. That is, we looked at detecting between Irish and English, Welsh and English 
and Breton and French.

\subsection{Corpus}
\vspace{-0.152cm}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r}
\hline
\multirow{2}{*}{\textbf{Pair}} & \multirow{2}{*}{\textbf{Language}}  & \multicolumn{2}{c|}{\textbf{Statistics} (\%)} \\\cline{3-4}
                &  &  Tokens & Segments \\
\hline
\multirow{2}{*}{Irish---English} & Irish & 332 & 40 \\
                                 & English & 379 & 42 \\
\hline
\multirow{2}{*}{Welsh---English} & Welsh & 419 & 64 \\
                                 & English & 378 & 66  \\
\hline
\multirow{2}{*}{Breton---French} & Breton & 388 & 54 \\
                                 & French & 379 & 53  \\
\hline
\end{tabular}
\end{center}
\caption{Document statistics of the annotated data used. }
\label{table:datastats}
\end{table}

We hand-annotated a small evaluation set from a selection of posts to a popular microblogging site.\footnote{\url{http://indigenoustweets.blogspot.in/2013/12/mapping-celtic-twittersphere.html}} The \emph{tweets} (microblog posts) were filtered into three sets which had been identified as Irish, Welsh and Breton using the \emph{langid} tool \cite{lui2012langid}. From these, we manually selected between 40 and 50 tweets for each language pair. Statistics on the number of segments and tokens is presented in Table~\ref{table:datastats}.
Certain tokens were escaped from the data, such as the `mentioned' character (\texttt{@} symbol), subject tags `hashtags' which are preceded by a \texttt{\#} symbol, hyperlinks and the sequence \texttt{rt} which stands for `re-tweet'. An example of the content of our corpus after hand annotation is given in Figure~\ref{fig:tweets}. All of the tweets had at least one instance of code-switching.

\begin{figure}
\begin{small}
\begin{alltt}
[\textbf{en} You're a] [\textbf{ga} Meirice\'{a}nach, c\'{e}n f\'{a}th] [\textbf{en} are you] [\textbf{ga} foghlaim Gaeilge?!] 
@afaltomkins [\textbf{cy} gorfod cael bach o tan] [\textbf{en} though init] 
[\textbf{en} omg] [\textbf{cy} mar cwn bach yn] [\textbf{en} black and tan] [\textbf{cy} a popeth,] [\textbf{en} even cuter!!] 
\end{alltt}
\end{small}
\caption{Example of text from a microblogging site chunked manually.}
\label{fig:tweets}
\end{figure}



\subsection{Alphabet n-gram approach}
\vspace{-0.152cm}
\label{alphan}

We use the character n-gram approach along with some heuristics which are relevant to our problem domain of identifying
segments for subsequent processing. We would like 
to both predict the code switched points but looking at the surrounding structure also decide the inclusion of them 
into the current or the next segment.
%\footnote{The software used in this experiment is available online at: \url{removed for review}.}

We first built character language models using IRSTLM \cite{federico08a} for the five languages in question. For English
and French a model was trained using the EuroParl corpus \cite{koehn2005europarl}. 
For Breton, Welsh and Irish we used corpora of text crawled from the web. To ensure no bias and also since our dataset for Breton was around 1.5 million, we sampled the same size of data for all the five languages. 
%For each of the languages we sampled 1.5 million words.
%The accuracy of the results saturate after the size of the dataset is around 1 million, and also since our collected corpus for Breton was around 1.5 million words we chose the limit of the sample to be this which also ensures no bias in the sampling of the languages. \\
In order to build a character language model %as opposed to the standard word-ngram based model %
we replaced spaces with the underscore 
symbol `\_', and then placed a space character between each character. Punctuation and non-letter characters are also 
part of this language model. For example, the word `sl\'{a}inte!' would be broken down into a sequence 
of \{`\_ s', `s l', `l \'{a}', `\'{a} i', `i n', `n t', `t e', `e !', `! \_'\}. 

%Then the probability for the alphabet sequence is calculated 
%for each language to be used in the experiment. 
%Improved Kneser-Ney is used for smoothing, and this smoothing approach would be useful 
%for higher-grams. \cite{kneser1995improved} \\

\subsection{Sequence chunking}
\vspace{-0.152cm}
\label{cschunking}

This section describes the way we apply heuristics to segment and label the input text.
%the heuristics taken into account while performing the chunking of the input data according to 
%the sequences of segments in different languages. 
In Figure ~\ref{fig:chunkal}, `chunks' represent the list of evaluated tuples of segments and their labelled language, `buffer' is the expanding segment.
 \textit{lang\_predict} corressponds to any function which is used to determine the language of the token. The flag 
variable helps in implementing the heuristic of minimum segment size while labelling chunks.
\begin{algorithm}
\caption*{\textsc{}}
%\caption{Chunking algorithm}\label{euclid}
{\fontsize{9}{9}\selectfont
\begin{algorithmic}[1]
%\Statex{}%Procedure{}{}
\State $\textit{buffer} = [\ ] \text{\ \ \  \#undecided expanding window of chunk}$
\State $\textit{chunks} = [\ ] \text{\ \ \  \#decided labelled segment}$
\State $\textit{buffer\_language} \gets \textit{lang\_predict}\text{(sentence[0])} \text{\ \ \ \#Language of first word}$
\State $\textit{flag} \gets \text{0}$
%\State $\text{for \ } \textit{word \ } \text{in \ } \textit{sentence:} $
\For{\texttt{word in sentence}}
\If {$ \textit{lang\_predict(word)} \text{==} \textit{buffer\_language} $}
        \If {$ \textit{flag} \text{=\ =\ 1} $}
        \State $\textit{buffer} += [\textit{word\_d,word}] $
        \State $\textit{flag} \gets \text{0}$
        \Else
        \State $\textit{buffer} += [\textit{word}] $
        \EndIf
        \EndIf
\If {$ \textit{lang\_prediction(word)} \text{!=} \textit{buffer\_langugae} $}
        \If {$ \textit{flag} \text{=\ =\ 0} $}
        \State $\textit{flag} \gets \text{1}$
        \State $\textit{word\_d} \gets \textit{word} $
        \State $\text{continue}$

        \Else
        \State $\textit{chunk} \text{+= \ }  \textit{[(buffer,buffer\_language)]} $
        %\State $\textit{buffer} \gets [] $
        \State $\textit{buffer} = [\textit{word\_d,word}] $
        \State $\textit{buffer\_language} = \gets \textit{lang\_predict}\text{(word)}$% \text{\#Language of new expanding chunk}$
        \State $\textit{flag} \gets \text{0}$
        \EndIf

\EndIf
\EndFor
\If {$\textit{length(buffer)} \text{!=0\ } $}
        \State $\textit{chunk} \text{+= \ }  \text{[\textit{[(buffer,buffer\_language)]}} $

\EndIf
%\EndProcedure
%\Statex{}
\end{algorithmic}}
\end{algorithm}
\vspace{-0.4cm}
\begin{figure}[hb]
\def\@fs@post{}
\caption{Chunking Algorithm}
\label{fig:chunkal}
\end{figure}
\vspace{-0.4cm}
%A flag is set at the beginning of the input text, and this moves forward to the position in the text upto where confident 
%chunking has been performed. For example, in case of a continuous prediction there would be no change and since the 
%predicted tag of the new word corresponds to that already of the expanding chunk, this word is also included in the 
%same chunk. In case of a doubt, the flag is set at the same position, and no final decision on labelling the word is 
%performed, to have more confidence on the code-switch, the process moves on to the next token, if it corresponds to 
%that of the previous chunk, then both these undecided words are labelled and included in the chunk, otherwise a change 
%is noted and a new chunk is created with the changed label. In this process, we are trying to identify the sub-segments 
%that we need, thus performing both tasks at once, the first being language prediction and second being the chunking decision. 

\subsection{Word-based prediction}
\vspace{-0.152cm}
\label{wbased}

Designed keeping in mind importance of the most common words, this procedure included 
%This is a simple heuristic which was designed on the basis of the the most common words in the wordlists of the languages 
%which are in question. After 
checking each word against both of the word lists in question,\footnote{For the wordlists we used the \emph{aspell}
wordlists widely available on Unix systems.} it is associated to one language or another. In 
case of a conflict, for example, when the word exists in both wordlists, or in the case that it is unknown to both, the option 
of continuing with the previous span was taken and the previous selected tag was labelled, thus increasing the chunk. 

\subsection{Word-based prediction with character backoff}
\vspace{-0.152cm}
\label{wbasedbackoff}

%A better way to predict the spans of the sub-segments of the text is to include the two methods of word and character-based techniques 
%as described above.
In case of the word being present in only one of the two monolingual word lists the classification is simple, 
but in case of a conflict, a character bigram backoff was introduced to help us disambiguate the language label. 
%This method works well because the earlier heuristic approach of just labelling the word with the label of the span which is expanding, would mean less code-switch detection and more shift towards the majority class. 


% issues:
%% proper nouns
%% other language interjections (fáilte), diolch, diolch yn fawr
%% spelling
%% abbreviations


\vspace{-0.132cm}
\section{Evaluation}
\vspace{-0.132cm}
\label{sec:eval}

For the Evaluation procedure, we follow the footsteps of the CoNLL-2000 shared task on language-independent 
named entity recognition: dividing text into syntactically related non-overlapping groups of words. This 
chunking mechanism \cite{tjong2003introduction} is very similar to ours, in terms of words which only belong to 
one category (here, language), and also evaluation based on the segment structure present in the data. The chunks 
here are such that they belong to only one language. 

%For our current task, each of the texts have been limited to two languages i.e. the primary language (the Celtic language) and the 
%secondary language. % (the `majority' language).
%Hence the type of chunking tags are limited to these. 
The evaluation statistics shown 
in Tables~\ref{table:precisionrecall}~and~\ref{table:accuracy} mention two values for each of the experiment conducted on the three bilingual 
language datasets. The first, is the percentage of correctly detected phrases, which is the overall precision and the second is the 
number of phrases in the data that were found by the chunker, which is the overall recall. 

Apart from the techniques discussed in section~\ref{sec:method}, some baselines are also used to give a comparative view of how well all the mechanisms perform.

\subsection{Baseline}
\vspace{-0.132cm}
\label{baseline}

%This is the most na\"{i}ve method of classification, 
We used the language identification tool \texttt{langid.py} \cite{lui2012langid} on the whole dataset and labelled all the individual lines according to this majority classification. As no chunking is performed we can expect that the precision and recall will be very low. However it does provide a reasonable baseline for the per-word accuracy.  

\subsection{\texttt{langid} character trigram prediction}
\vspace{-0.132cm}
\label{langidstuff}


For this system we used the character trigram probabilities to predict the detected language for each token. Trigrams were chosen after experimenting with 1--5 grams. The heuristics in section~\ref{sec:method} were followed for the text processing and chunking part of the method.

\section{Results}
\label{sec:results}
As described in Section~\ref{sec:method} the data collected from Twitter for the three language pairs, was processed using the techniques mentioned. The statistics of the same are given in Table~\ref{table:datastats}. 

\begin{table}
\begin{center}
\begin{tabular}{|lc|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{\textbf{System}}            & & \multicolumn{2}{c}{\textbf{Irish---English}} & \multicolumn{2}{|c|}{\textbf{Welsh---English}} & \multicolumn{2}{c|}{\textbf{Breton---French}}  \\\cline{3-8}
                                          &      &  Irish &  English & Welsh  & English & Breton & French \\
\hline 
\multirow{2}{*}{\texttt{baseline}}        &  $p$ &  2.50   & 0.0      & 0.0   & 0.0 & 0.0 & 0.0 \\
                                          & $r$  & 2.56    & 0.0      & 0.0   & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid-3character}}         &  $p$ &  5.00   & 14.29    & 0.0   & 21.21 & 1.85 & 20.75 \\
                                          & $r$  & 5.41    & 8.45     & 0.0   & 14.58 & 1.92 & 12.36 \\
\hline
\multirow{2}{*}{\texttt{wordlist}}        &  $p$ &  32.50 & 28.57     & 26.69 & {\bf 40.91} & 57.41 & 33.96 \\
                                          & $r$  & 23.64  & 26.09     & {\bf 26.03} & {\bf 33.75} & 47.69 & 33.33 \\
\hline
\multirow{2}{*}{\texttt{character bigram}}          &  $p$ &  32.50   & 35.71   & 23.44 & 19.70  & 57.41 & 52.83 \\
                                          & $r$  & 22.41    & 26.79   & 15.31 & 16.67 & 41.33 & 37.84 \\
\hline
\multirow{2}{*}{\texttt{wordlist+character bigram}} &  $p$ &  {\bf 52.50}   & {\bf 50.00}   & {\bf 32.81} & 31.82 & {\bf 70.37} & {\bf 67.92} \\
                                          & $r$  & {\bf 38.18}    & {\bf 43.75}   & 24.14 & 25.61 & {\bf 57.58} & {\bf 57.14} \\
\hline
\end{tabular}
\end{center}
\vspace{-0.4cm}
\caption{Precision, $p$ and recall, $r$ for the systems by language.}
\label{table:precisionrecall}

\end{table}


\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
\multirow{2}{*}{\textbf{System}} &  \multicolumn{3}{c|}{\textbf{Accuracy} (\%)} \\\cline{2-4}
       &   Irish---English & Welsh---English & Breton---French \\ 
\hline
\texttt{baseline} & 42.76 & 42.16 & 44.07 \\
\hline
\texttt{langid-3character} & 57.24 & 45.92 & 43.16 \\
\hline
\texttt{wordlist} & 79.75 & \textbf{74.28} & 83.96 \\
\hline
\texttt{character bigram} & 81.29 & 65.62 & 76.79 \\
\hline
\texttt{wordlist+character bigram} & \textbf{85.79} & 72.40 & \textbf{88.79} \\
\hline
\end{tabular}
\end{center}
\caption{Accuracy of the systems over the three language pairs. The accuracy measures how often a token
  was assigned to the right language, independent of span.}
\label{table:accuracy}
\end{table}
%In terms of per-word accuracy, the baseline performs as expected, given that half of the words in the test set are from a given language. 
%The \texttt{langid.py} based methods allow for segmenting the text into chunks, but favour the majority language.
%\footnote{This could be because of the default models being used.} 
While the precision and recall are low for the remaining models, we see that we are able to improve 
The performance by combining the wordlist based model with a character bigram model. And what is more, we are able to begin to not only 
identify particular words in a language, but also segments.

\vspace{-0.132cm}
\section{Conclusions}
\vspace{-0.132cm}
\label{sec:conclusions}

This paper has presented a very preliminary investigation into subsegment language identification in Celtic language texts. We have
proposed a model that chunks input text into segments and performs language identification on these segments at the same time. Precision and
recall are low, leaving a lot of room for further work.
% Some of our plans are as follows. We would like to annotate more test data,
%at least 100 tweets in each language.
 Although In \cite{king2013labeling}, the authors label on a per word level, yet we would like to include supervised methods and features talked about in this research to improve our efficiency while dealing with segments. We would also like to attempt our method using higher order character n-gram models for backoff, and n-gram word language models for detection and on more annotated data. 

% future work

%% bigger and better test data, and distributable
%% better evaluation -- significance testing
%%  evaluation by application
%% try it with scots gaelic
%% 
%% mikel's idea
%% higher order character n-gram models

\section*{Acknowledgements}
\vspace{-0.132cm}

We thank Kevin Scannell for help with providing data from Twitter and for useful comments and suggestions. This work
was undertaken as part of the Apertium project in the 2014 Google Summer of Code.

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{codeswitch_camera_ready}


\end{document}
