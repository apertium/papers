%Try1: akshay minocha
%
% File coling2014.tex
%
% Contact: jwagner@computing.dcu.ie

\documentclass[11pt]{article}
\usepackage{coling2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage[small,bf]{caption}
\usepackage{multirow}

%\setlength\titlebox{5cm}

\title{Subsegmental language detection in Celtic language text}

\author{Akshay Minocha \\
  IIIT Hyderabad  \\
  Hyderabad (India) \\
  {\small {\tt akshay.minocha@students.iiit.ac.in}} \\\And
  Francis M. Tyers \\
  Giellatekno / CLEAR \\
  UiT Norgga \'arktala\v{s} universitehta  \\
  9017 Romsa (Norway) \\
  {\small {\tt francis.tyers@uit.no}} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper describes an experiment to detect the language of subsegments
  of text in three Celtic languages: Breton, Irish and Welsh. 
\end{abstract}

\section{Introduction}
\label{intro}

Determining the language of a piece of text is one of the first steps that must be taken
before proceeding with further computational processing. This task has received a substantial amount of
attention in recent years (\cite{cavnar1994n},\cite{lui2012langid}). However, previous research has on the whole assumed
that a given text will be in a single language. When dealing with text from formal domains,
this may be the case --- although there are exceptions --- such as quotations embedded in
the text in another language. But when dealing with informal text, particularly in languages
where the speech community is predominantly bi- or multi-lingual, this assumption may not hold.

There are several ways in which an informal text may contain parts in different languages, such as
code switching, quotations, named entities, interjections, translations, etc. Some of these are presented
in Table~\ref{table:examples}

\begin{table}
\begin{tabular}{ll}
 \textbf{Code switching}: & You're a [Meirice\'{a}nach, c\'{e}n f\'{a}th] are you [foghlaim Gaeilge?!] \\
\textbf{Quotations}: & And then he said ` ' \\
 \textbf{Named entities}: & [Dr Jekyll] ha [Mr Hyde] embannet gant [\'{E}ditions Aber] \\
 \textbf{Interjections}: & Hey, that's great, [diolch yn fawr!] \\
 \textbf{Translations}: & Bloavezh mat d'an holl ! [Bonne ann\'{e} \`{a} tous !] \\
\end{tabular}
\label{table:examples}
\caption{Some examples of text segments containing more than one language}
\end{table}

The work presented in this paper was motivated by the problems in normalising non-standard input
for the Celtic languages as a precursor to machine translation. When applying a normalisation 
strategy to a piece of text, it is necessary to first know the language of the piece of text you 
are applying it to.

The remainder of the paper is laid out as follows. In section~\ref{sec:method} we describe the problem
in more detail and look at relevant prior work before proposing a novel method of sub-sentential
language detection. Section~\ref{sec:eval} describes the evaluation methodology. Then in section~\ref{sec:results}
we present the results of our method and compare it against several other possible methods. Finally, section~\ref{sec:conclusions}
presents future work and conclusions.

\section{Related Work}
\label{sec:relwork}

There has been significant research on code-switching and segment detection problems. The good thing about multilingual code switched data is that there are differences in the words of different languages - phonologically, morphologically, lexically and syntactically. At least that is the case with the pairs of languages we have chosen according to their usage in common day scenario. \\

Before the Internet era, this study was focussed more on speech data. Initial research on this code switching was done for Marathi and English \cite{joshi1982processing} which included notable work about the formalisms and parsing. \\

It is seen that language modelling techniques have shown promise earlier, such as in \cite{yu2013identification} the experiment on Mandarin-Taiwanese code-switched sentences show a high accuracy in terms of detecting code-switched sentences. \\

Our idea of going to character level inspection has been inspired both by the language modelling techniques used by the community as well as the acoustic modelling research. From \cite{chan2004detection} which has made use of the bi-phone probabilities and calculated them to measure a confidence metric, to \cite{lyu2006language} which has made use of a classification method, named syllable-based duration classification, which uses the tonal syllable properties along with the speech signals to help predict the code switch points. \cite{yeong2010language} uses syllable structure information, and increases our confidence is using the character n-gram approach for detection of the code switching points, their study was for the Malay-English, identifying the language of the words in the code switched data set.\\ 

\section{Methodology}
\label{sec:method}
We use the character n-gram approach along with some heuristics to mainly define the first task at hand. We would like to both predict the code switched points but looking at the surrounding structure also decide the inclusion of them into the current or the next segment. \\


Alphabet n-gram approach - 

To predict code switch points we needed some raw text to train our character language models on. For English and French, we used the help of the Europarl \cite{koehn2005europarl}, for Breton, Welsh and Irish, data from credible sources on web were fetched. The size of the dataset for all the language sources was about 1.5million words. \\
Building the character language models was undertaken using IRSTLM \cite{federico08a}, the input characters are to be introduced in the format of token separated by space and blank spaces replaced by `\_'. For example, `sl\'{a}inte' would be broken down into a set which is \{`\_ s', `s l', `l \'{a}', `\'{a} i', `i n', `n t', `t e', `e \_'\}.Then the probability for the alphabet sequence is calculated for each language pair, to be used in the experiment. Improved kneser-ney is used for smoothing, and this smoothing approach would be useful for higher-grams. \cite{kneser1995improved} \\

Once we get the probabilities, our task at hand is to take in the data and process it such that the  sub-segments of code-switched data are detected in it. We collected data from twitter over a period of time and this was filtered into three sets which had been identified to be Irish, Welsh and Breton. To be able to compare with the language model built above in the character n-gram approach, this data is to be processed like it aswell. \\

Since, there were instances of non-standard input in the Twitter data, certain terminologies were escaped, such as mentions (starting with ‘@’), hashtags (\#xyz), Re-Tweet (RT) and hyperlinks.\\
This processed data is then given as input to our module which uses heuristics to come up with sub-segment information. \\

Code-Switch Chunking - \\

This section describes the rules taken into account while performing the chunking of the input data according to the code-switch prediction. \\

A flag is set at the beginning, and this moves forward to the position in the text upto where confident chunking has been performed. For example, in case of a continuous prediction there would be no change and since the predicted tag of the new word corresponds to that already of the expanding chunk, this word is also included in the same chunk. In case of a doubt, the flag is set at the same position, and no final decision on labelling the word is performed, to have more confidence on the code-switch, the process moves on to the next token, if it corresponds to that of the previous chunk, then both these undecided words are labelled and included in the chunk, otherwise a change is noted and a new chunk is created with the changed label. In this process, we are trying to identify the sub-segments that we need, thus doing two tasks together and optimally, one is the prediction and second is the decision on chunking. \\



% issues:
%% proper nouns
%% other language interjections (fáilte), diolch, diolch yn fawr
%% spelling
%% abbreviations


%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
%

\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version, see
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licenced under a Creative Commons 
    % Attribution 4.0 International License.
    % Page numbers and proceedings footer are added by
    % the organizers.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
}

\section{Evaluation}
\label{sec:eval}

For the Evaluation procedure, we follow the footsteps of the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words. This chunking mechanism \cite{tjong2003introduction} is very similar to ours, in terms of words which only belong to one category (here, language), and also evaluation based on the phrase structure present in the data. The chunks here are such that they belong to only one language. For example, the sentence below, shows the chunks as group of words in square brackets and their tagged language at the beginning in braces - \\

\begin{figure}
\begin{small}
\begin{alltt}
@steffdafydd [\textbf{cy} gorfod cael bach o tan] [\textbf{en} though init]
[\textbf{en} omg] [\textbf{cy} mar cwn bach yn] [\textbf{en} black and tan] [\textbf{cy} a popeth,] [\textbf{en} even cuter!!]
\end{alltt}
\end{small}
\label{fig:tweets}
\caption{Example of text from a microblogging site chunked}
\end{figure}

For our current task, each of the texts have been limited to two languages i.e. the primary and the secondary language. Hence the type of chunking tags are limited to these. The evaluation statics shown in the Table mention two values for each of the experiment conducted on the three bilingual language datasets. The first, is the percentage of correctly detected phrases, which is the overall precision and the second is the number of phrases in the data that were found by the chunker, which is the overall recall. \\

The different techniques used for the sub-segmenting task are described as follows - 
1. Baseline - This is the most naive method of classification, we used the language identification tool langid.py \cite{lui2012langid} on the whole dataset and labelled all the individual lines according to this single major classification. \\
2. Langid character bigram and trigram prediction - After restricting the predicted languages to be amongst the two in the dataset, we used the character bi-gram and then tri-gram probabilities to predict the detected language for each token, some rules like in Methodology were followed which included that the span of a segment or a code-switched phrase is more than a word, the social media non-language entities such as the hashtags, usernames and urls do not make a difference. \\
3. Word based prediction - This is a simple heuristic which was designed on the basis of the the most common words in the wordlists of the languages which are in question. After checking each word against one of the word lists, it was associated to that particular language. In case of a confusion, for example, when the word exists in both the word lists or not even in one, the option of continuing with the previous span was taken and the previous selected tag was labelled, thus increasing the chunk. \\
4. Character bigram - This method is in line with what has been described in the Methodology section \\
5. Word based Prediction with Character backoff - A better way to predict the spans of the sub-segments of the text is to include the two methods of word and character based techniques as described above. In case of the word being present in only one of the two monolingual word lists the classification is simple, but in case of a confusion as described in 4. above, a character bigram backoff was introduced to help us disambiguate the language label. This method works well because the earlier heuristic approach of just labelling the word with the label of the span which is expanding, would mean less code-switch detection and more shift towards baseline. \\


\section{Results}
\label{sec:results}
As described in the evaluation the techniques were used on a dataset collected over a period of time, for the three language pairs\footnote{\url{http://indigenoustweets.blogspot.in/2013/12/mapping-celtic-twittersphere.html}}. The statics of the same are described in the Table below. // 

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r}
\hline
\multirow{2}{*}{\textbf{Pair}} & \multirow{2}{*}{\textbf{Language}}  & \multicolumn{2}{c|}{\textbf{Statistics} (\%)} \\\cline{3-4}
                &  &  Tokens & Segments \\
\hline
\multirow{2}{*}{Irish---English} & Irish & 332 & 40 \\
                                 & English & 379 & 42 \\
\hline
\multirow{2}{*}{Welsh---English} & Welsh & 419 & 64 \\
                                 & English & 378 & 66  \\
\hline
\multirow{2}{*}{Breton---French} & Breton & 388 & 54 \\
                                 & French & 379 & 53  \\
\hline
\end{tabular}
\end{center}
\label{table:accuracy}
\caption{Document Statistics of the annotated data used.}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|lc|r|r|r|r|r|r|}
\hline
\multirow{2}{*}{\textbf{System}}            & & \multicolumn{2}{c}{\textbf{Irish---English}} & \multicolumn{2}{|c|}{\textbf{Welsh---English}} & \multicolumn{2}{c|}{\textbf{Breton---French}}  \\\cline{3-8}
                                          &      &  Irish &  English & Welsh  & English & Breton & French \\
\hline 
\multirow{2}{*}{\texttt{baseline}}        &  $p$ &  2.50   & 0.0      & 0.0   & 0.0 & 0.0 & 0.0 \\
                                          & $r$  & 2.56    & 0.0      & 0.0   & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid2}}         &  $p$ &  0.00   & 7.14     & 0.0   & 18.18    & 0.0 & 28.30 \\
                                          & $r$  & 0.00    & 5.00     & 0.0   & 14.12 & 0.0 & 18.99 \\
\hline
\multirow{2}{*}{\texttt{langid3}}         &  $p$ &  5.00   & 14.29    & 0.0   & 21.21 & 1.85 & 20.75 \\
                                          & $r$  & 5.41    & 8.45     & 0.0   & 14.58 & 1.92 & 12.36 \\
\hline
\multirow{2}{*}{\texttt{wordlist}}        &  $p$ &  32.50 & 28.57     & 26.69 & {\bf 40.91} & 57.41 & 33.96 \\
                                          & $r$  & 23.64  & 26.09     & {\bf 26.03} & {\bf 33.75} & 47.69 & 33.33 \\
\hline
\multirow{2}{*}{\texttt{bigram}}          &  $p$ &  32.50   & 35.71   & 23.44 & 19.70  & 57.41 & 52.83 \\
                                          & $r$  & 22.41    & 26.79   & 15.31 & 16.67 & 41.33 & 37.84 \\
\hline
\multirow{2}{*}{\texttt{wordlist+bigram}} &  $p$ &  {\bf 52.50}   & {\bf 50.00}   & {\bf 32.81} & 31.82 & {\bf 70.37} & {\bf 67.92} \\
                                          & $r$  & {\bf 38.18}    & {\bf 43.75}   & 24.14 & 25.61 & {\bf 57.58} & {\bf 57.14} \\
\hline
\end{tabular}
\end{center}
\label{table:precisionrecall}
\caption{Precision, $p$ and recall, $r$ for the systems by language.}

\end{table}


\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
\multirow{2}{*}{\textbf{System}} &  \multicolumn{3}{c|}{\textbf{Accuracy} (\%)} \\\cline{2-4}
       &   Irish---English & Welsh---English & Breton---French \\ 
\hline
\texttt{baseline} & 42.76 & 42.16 & 44.07 \\
\hline
\texttt{langid2} & 50.35 & 42.28 & 45.11  \\
\hline
\texttt{langid3} & 57.24 & 45.92 & 43.16 \\
\hline
\texttt{wordlist} & 79.75 & \textbf{74.28} & 83.96 \\
\hline
\texttt{bigram} & 81.29 & 65.62 & 76.79 \\
\hline
\texttt{wordlist+bigram} & \textbf{85.79} & 72.40 & \textbf{88.79} \\
\hline
\end{tabular}
\end{center}
\label{table:accuracy}
\caption{Accuracy of the systems over the three language pairs. The accuracy measures how often a token
  was assigned to the right language, independent of span.}
\end{table}


(CHECKLATER-FORMATTING)

The Results clearly show that the baseline has a negligible precision and recall, this means that the task of detecting sub-segments is complex and there needs to be more improvement in the technique. You can see that langid.py is better suited for document classification and that when such varied content is introduced the result is an improvement but not promising. In case of the character bigram prediction technique and the word based chunking method, the results show a significant increase in precision and recall values from the rest. The last in the list is the word based prediction with character backoff, which performs collectively better than the others with a high accurracy margin. \\


\section{Conclusions}
\label{sec:conclusions}

% future work

%% bigger and better test data, and distributable
%% better evaluation -- significance testing
%%  evaluation by application
%% try it with scots gaelic
%% 
%% mikel's idea
%% higher order character n-gram models

\section*{Acknowledgements}

We thank Kevin Scannell... 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{codeswitch}


\end{document}
