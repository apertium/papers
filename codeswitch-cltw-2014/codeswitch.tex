%Try1: akshay minocha
%
% File coling2014.tex
%
% Contact: jwagner@computing.dcu.ie
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{coling2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage[small,bf]{caption}
\usepackage{multirow}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Subsegmental language detection in Celtic language text}

\author{Akshay Minocha \\
  IIIT Hyderabad  \\
  Hyderabad (India) \\
  {\small {\tt akshay.minocha@students.iiit.ac.in}} \\\And
  Francis M. Tyers \\
  Giellatekno / CLEAR \\
  UiT Norgga \'arktala\v{s} universitehta  \\
  9017 Romsa (Norway) \\
  {\small {\tt francis.tyers@uit.no}} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper describes an experiment to detect the language of subsegments
  of text in three Celtic languages: Breton, Irish and Welsh. 
\end{abstract}

\section{Introduction}
\label{intro}
With the increase in multilingualism and the adoption of ‘another’ acceptable language apart from the usual creates a change in the data. This change can be looked upon as bad when we are faced with the burden of upgrading systems to adapt to it, on the other hand such change is 
unavoidable and has to be welcomed. The good part is that the user or the speaker feels more comfortable in expressing their opinions. Such change directly requires us to upgrade our current systems \cite{fung2008multilingual} so that the processing or the analysis of the data is in no way compromised.\\
   Such code switching is more common in less formal ways of communication like on chats, forums and social media. Our motivation is to build a detection scheme which can sub-segment such sentences to the primary and secondary language tags and then prepare this to be processed further by modules which make use of this information to analyse useful information from this data. Specially in the case of languages like Irish, Breton and Welsh where the native speakers are very less, these people reach out to others and most of their posts are code-switched. Rather than promoting research based on monolingual technologies we would like to adopt to the trend and try to help other researchers in understanding this data in a better way.

%   In Ireland, and the surrounding areas a significant number of speakers use Welsh, Breton, Irish, English and French. About 14.6\% of the population of Wales considers themselves to be fluent in Welsh(2011) \footnote{http://ons.gov.uk/ons/rel/census/2011-census/key-statistics-for-unitary-authorities-in-wales/stb-2011-census-key-statistics-for-wales.html#tab---Proficiency-in-Welsh}. In the Republic of Ireland 94,000 people use it as a primary language outside the education system \footnote{http://www.cso.ie/en/media/csoie/census/documents/census2011pdr/Pdf\%208\%20Tables.pdf}. It is noticed that the Irish and the Welsh people often use English with their respective languages while they code-switch while Bretons have a natural French influence. Hence, in our experiments we plan to analyse the results for code switching and detection of segments in the following bilingual pairs - Welsh-English, Irish-English and Breton-French,  thus identifying the primary and secondary languages in the data.
\cite{lyu2006}

\begin{figure}

\end{figure}

% issues:
%% proper nouns
%% other language interjections (fáilte), diolch, diolch yn fawr
%% spelling
%% abbreviations


%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
%

\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version, see
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licenced under a Creative Commons 
    % Attribution 4.0 International License.
    % Page numbers and proceedings footer are added by
    % the organizers.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
}

\section{Evaluation}
For the Evaluation procedure, we follow the footsteps of the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words. This chunking mechanism \cite{tjong2003introduction} is very similar to ours, in terms of words which only belong to one category (here, language), and also evaluation based on the phrase structure present in the data. The chunks here are such that they belong to only one language. For example, the sentence below, shows the chunks as group of words in square brackets and their tagged language at the beginning in braces - \\

\begin{figure}
\begin{small}
\begin{alltt}
@steffdafydd [\textbf{cy} gorfod cael bach o tan] [\textbf{en} though init]
[\textbf{en} omg] [\textbf{cy} mar cwn bach yn] [\textbf{en} black and tan] [\textbf{cy} a popeth,] [\textbf{en} even cuter!!]
\end{alltt}
\end{small}
\label{fig:tweets}
\caption{Example of text from a microblogging site chunked}
\end{figure}

For our current task, each of the texts have been limited to two languages i.e. the primary and the secondary language. Hence the type of chunking tags are limited to these. The evaluation statics shown in the Table mention two values for each of the experiment conducted on the three bilingual language datasets. The first, is the percentage of correctly detected phrases, which is the overall precision and the second is the number of phrases in the data that were found by the chunker, which is the overall recall. \\

The different techniques used for the sub-segmenting task are described as follows - 

1. Baseline - This is the most naive method of classification, we used the language identification tool langid.py \cite{lui2012langid} on the whole dataset and labelled all the individual lines according to this single major classification. \\
2. Langid character bigram and trigram prediction - After restricting the predicted languages to be amongst the two in the dataset, we used the character bi-gram and then tri-gram probabilities to predict the detected language for each token, some rules like in Methodology were followed which included that the span of a segment or a code-switched phrase is more than a word, the social media non-language entities such as the hashtags, usernames and urls do not make a difference. \\
3. Word based prediction - This is a simple heuristic which was designed on the basis of the the most common words in the wordlists of the languages which are in question. After checking each word against one of the word lists, it was associated to that particular language. In case of a confusion, for example, when the word exists in both the word lists or not even in one, the option of continuing with the previous span was taken and the previous selected tag was labelled, thus increasing the chunk. \\
4. Character bigram - This method is in line with what has been described in the Methodology section \\
5. Word based Prediction with Character backoff - A better way to predict the spans of the sub-segments of the text is to include the two methods of word and character based techniques as described above. In case of the word being present in only one of the two monolingual word lists the classification is simple, but in case of a confusion as described in 4. above, a character bigram backoff was introduced to help us disambiguate the language label. This method works well because the earlier heuristic approach of just labelling the word with the label of the span which is expanding, would mean less code-switch detection and more shift towards baseline. \\


\section{Results}

\begin{table}
\begin{center}
\begin{tabular}{|lc|rr|rr|rr|}
\hline
            & & \multicolumn{2}{c}{Irish---English} & \multicolumn{2}{c}{Welsh---English} & \multicolumn{2}{c}{Breton---French}  \\
\hline
                                        &         &  Irish &       English & Welsh & English & Breton & French \\
\multirow{2}{*}{\texttt{baseline}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid2}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid3}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{wordlist}} &  $p$ &  32.50 & 28.57        & 26.69 & 40.91 & 57.41 & 33.96 \\
                                       & $r$  & 23.64   & 26.09       & 26.03 & 33.75 & 47.69 & 33.33 \\
\hline
\multirow{2}{*}{\texttt{bigram}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{wordlist+bigram}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\end{tabular}
\end{center}

\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{System} &  \multicolumn{3}{c|}{\textbf{Accuracy} (\%)} \\
\hline
       &   Irish---English & Welsh---English & Breton---French \\ 
\hline
\texttt{baseline} & 42.76 & 42.16 & 94.65 \\
\hline
\texttt{langid2} & 50.35 & 42.28 & 45.11  \\
\hline
\texttt{langid3} & 57.24 & 45.92 & 43.16 \\
\hline
\texttt{wordlist} & 79.75 & 74.28 & 83.96 \\
\hline
\texttt{bigram} & 81.29 & 65.62 & 76.79 \\
\hline
\texttt{wordlist+bigram} & 85.79 & 72.40 & 88.79 \\
\hline
\end{tabular}
\end{center}
\label{table:accuracy}
\caption{Accuracy of the systems over the three language pairs}
\end{table}


(CHECKLATER-FORMATTING)
1. Baseline - Negligible precision and recall show that the task of detecting sub-segments in complex and there needs to be more improvement in the methodology. 


\section{Conclusions}


\section*{Acknowledgements}

We thank Kevin Scannell... 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{codeswitch}


\end{document}
