%Try1: akshay minocha
%
% File coling2014.tex
%
% Contact: jwagner@computing.dcu.ie

\documentclass[11pt]{article}
\usepackage{coling2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{alltt}
\usepackage[small,bf]{caption}
\usepackage{multirow}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Subsegmental language detection in Celtic language text}

\author{Akshay Minocha \\
  IIIT Hyderabad  \\
  Hyderabad (India) \\
  {\small {\tt akshay.minocha@students.iiit.ac.in}} \\\And
  Francis M. Tyers \\
  Giellatekno / CLEAR \\
  UiT Norgga \'arktala\v{s} universitehta  \\
  9017 Romsa (Norway) \\
  {\small {\tt francis.tyers@uit.no}} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This paper describes an experiment to detect the language of subsegments
  of text in three Celtic languages: Breton, Irish and Welsh. 
\end{abstract}

\section{Introduction}
\label{intro}

Determining the language of a piece of text is one of the first steps that must be taken
before proceeding with further computational processing. This task has received a substantial amount of
attention in recent years (X,Y,Z). However, previous research has on the whole assumed
that a given text will be in a single language. When dealing with text from formal domains,
this may be the case --- although there are exceptions --- such as quotations embedded in
the text in another language. But when dealing with informal text, particularly in languages
where the speech community is predominantly bi- or multi-lingual, this assumption may not hold.

There are several ways in which an informal text may contain parts in different languages, such as
code switching, quotations, named entities, interjections, translations, etc. Some of these are presented
in Table~\ref{table:examples}

\begin{table}
\begin{tabular}{ll}
 \textbf{Code switching}: & \\
\textbf{Quotations}: & And then he said ` ' \\
 \textbf{Named entities}: &``[Dr Jekyll] ha [Mr Hyde] embannet gant [Éditions Aber]'' \\
 \textbf{Interjections}: & ``Hey, that's great, [diolch yn fawr!]'' \\
 \textbf{Translations}: & ``Bloavezh mat d'an holl ! [Bonne année à tous !]'' \\
\end{tabular}
\label{table:examples}
\caption{Some examples of text segments containing more than one language}
\end{table}

The work presented in this paper was motivated by the problems in normalising non-standard input
for the Celtic languages as a precursor to machine translation. When applying a normalisation 
strategy to a piece of text, it is necessary to first know the language of the piece of text you 
are applying it to.

The remainder of the paper is laid out as follows. In section~\ref{sec:method} we describe the problem
in more detail and look at relevant prior work before proposing a novel method of sub-sentential
language detection. Section~\ref{sec:eval} describes the evaluation methodology. Then in section~\ref{sec:results}
we present the results of our method and compare it against several other possible methods. Finally, section~\ref{sec:conclusions}
presents future work and conclusions.

\section{Methodology}
\label{sec:method}

% issues:
%% proper nouns
%% other language interjections (fáilte), diolch, diolch yn fawr
%% spelling
%% abbreviations


%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under "final paper" for your variant of English.
%

\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    Place licence statement here for the camera-ready version, see
    Section~\ref{licence} of the instructions for preparing a
    manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licenced under a Creative Commons 
    % Attribution 4.0 International License.
    % Page numbers and proceedings footer are added by
    % the organizers.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
}

\section{Evaluation}
\label{sec:eval}
For the Evaluation procedure, we follow the footsteps of the CoNLL-2000 shared task: dividing text into syntactically related non-overlapping groups of words. This chunking mechanism \cite{tjong2003introduction} is very similar to ours, in terms of words which only belong to one category (here, language), and also evaluation based on the phrase structure present in the data. The chunks here are such that they belong to only one language. For example, the sentence below, shows the chunks as group of words in square brackets and their tagged language at the beginning in braces - \\

\begin{figure}
\begin{small}
\begin{alltt}
@steffdafydd [\textbf{cy} gorfod cael bach o tan] [\textbf{en} though init]
[\textbf{en} omg] [\textbf{cy} mar cwn bach yn] [\textbf{en} black and tan] [\textbf{cy} a popeth,] [\textbf{en} even cuter!!]
\end{alltt}
\end{small}
\label{fig:tweets}
\caption{Example of text from a microblogging site chunked}
\end{figure}

For our current task, each of the texts have been limited to two languages i.e. the primary and the secondary language. Hence the type of chunking tags are limited to these. The evaluation statics shown in the Table mention two values for each of the experiment conducted on the three bilingual language datasets. The first, is the percentage of correctly detected phrases, which is the overall precision and the second is the number of phrases in the data that were found by the chunker, which is the overall recall. \\

The different techniques used for the sub-segmenting task are described as follows - 

1. Baseline - This is the most naive method of classification, we used the language identification tool langid.py \cite{lui2012langid} on the whole dataset and labelled all the individual lines according to this single major classification. \\
2. Langid character bigram and trigram prediction - After restricting the predicted languages to be amongst the two in the dataset, we used the character bi-gram and then tri-gram probabilities to predict the detected language for each token, some rules like in Methodology were followed which included that the span of a segment or a code-switched phrase is more than a word, the social media non-language entities such as the hashtags, usernames and urls do not make a difference. \\
3. Word based prediction - This is a simple heuristic which was designed on the basis of the the most common words in the wordlists of the languages which are in question. After checking each word against one of the word lists, it was associated to that particular language. In case of a confusion, for example, when the word exists in both the word lists or not even in one, the option of continuing with the previous span was taken and the previous selected tag was labelled, thus increasing the chunk. \\
4. Character bigram - This method is in line with what has been described in the Methodology section \\
5. Word based Prediction with Character backoff - A better way to predict the spans of the sub-segments of the text is to include the two methods of word and character based techniques as described above. In case of the word being present in only one of the two monolingual word lists the classification is simple, but in case of a confusion as described in 4. above, a character bigram backoff was introduced to help us disambiguate the language label. This method works well because the earlier heuristic approach of just labelling the word with the label of the span which is expanding, would mean less code-switch detection and more shift towards baseline. \\


\section{Results}
\label{sec:results}
As described in the evaluation the techniques were used on a dataset collected over a period of time, for the three language pairs\footnote{\url{http://indigenoustweets.blogspot.in/2013/12/mapping-celtic-twittersphere.html}}. The statics of the same are described in the Table below. // 

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|r}
\hline
\multirow{2}{*}{\textbf{Pair}} & \multirow{2}{*}{\textbf{Language}}  & \multicolumn{2}{c|}{\textbf{Statistics} (\%)} \\\cline{3-4}
                &  &  Tokens & Segments \\
\hline
\multirow{2}{*}{Irish---English} & Irish & 332 & 40 \\
                                 & English & 379 & 42 \\
\hline
\multirow{2}{*}{Welsh---English} & Welsh & 419 & 64 \\
                                 & English & 378 & 66  \\
\hline
\multirow{2}{*}{Breton---French} & Breton & 388 & 54 \\
                                 & French & 379 & 53  \\
\hline
\end{tabular}
\end{center}
\label{table:accuracy}
\caption{Document Statistics of the annotated data used.}
\end{table}



\begin{table}
\begin{center}
\begin{tabular}{|lc|rr|rr|rr|}
\hline
            & & \multicolumn{2}{c}{Irish---English} & \multicolumn{2}{c}{Welsh---English} & \multicolumn{2}{c}{Breton---French}  \\
\hline
                                        &         &  Irish &       English & Welsh & English & Breton & French \\
\multirow{2}{*}{\texttt{baseline}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid2}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{langid3}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{wordlist}} &  $p$ &  32.50 & 28.57        & 26.69 & 40.91 & 57.41 & 33.96 \\
                                       & $r$  & 23.64   & 26.09       & 26.03 & 33.75 & 47.69 & 33.33 \\
\hline
\multirow{2}{*}{\texttt{bigram}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\multirow{2}{*}{\texttt{wordlist+bigram}} &  $p$ &  2.5 & 0.0        & 0.0 & 0.0 & 0.0 & 0.0 \\
                                       & $r$  & 2.5   & 0.0       & 0.0 & 0.0 & 0.0 & 0.0 \\
\hline
\end{tabular}
\end{center}

\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
\textbf{System} &  \multicolumn{3}{c|}{\textbf{Accuracy} (\%)} \\
\hline
       &   Irish---English & Welsh---English & Breton---French \\ 
\hline
\texttt{baseline} & 42.76 & 42.16 & 44.07 \\
\hline
\texttt{langid2} & 50.35 & 42.28 & 45.11  \\
\hline
\texttt{langid3} & 57.24 & 45.92 & 43.16 \\
\hline
\texttt{wordlist} & 79.75 & 74.28 & 83.96 \\
\hline
\texttt{bigram} & 81.29 & 65.62 & 76.79 \\
\hline
\texttt{wordlist+bigram} & 85.79 & 72.40 & 88.79 \\
\hline
\end{tabular}
\end{center}
\label{table:accuracy}
\caption{Accuracy of the systems over the three language pairs}
\end{table}


(CHECKLATER-FORMATTING)

The Results clearly show that the baseline has a negligible precision and recall, this means that the task of detecting sub-segments is complex and there needs to be more improvement in the technique. You can see that langid.py is better suited for document classification and that when such varied content is introduced the result is an improvement but not promising. In case of the character bigram prediction technique and the word based chunking method, the results show a significant increase in precision and recall values from the rest. The last in the list is the word based prediction with character backoff, which performs collectively better than the others with a high accurracy margin. \\


\section{Conclusions}
\label{sec:conclusions}

\section*{Acknowledgements}

We thank Kevin Scannell... 

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{codeswitch}


\end{document}
