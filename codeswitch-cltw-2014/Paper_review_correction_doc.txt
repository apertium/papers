Paper Review correction Doc

============================================================================
                            REVIEWER #1
============================================================================
The first reviewer says that he is excited about the paper and the results in the paper are useful.. but the accept reject score that he gives to our paper is the least in the list

(1) The paper cites some work on multilingual ID for speech, which isn't really
relevant here.         There has been some good work on this question for text also,
e.g. Baldwin, Lau and Lui "Automatic Detection and Language Identification of
Multilingual Documents" and refs therein.


- Automatic Detection and Language Identiﬁcation of Multilingual Documents
--This paper talks about use of multiple languages in a document
--We mentioned the speech papers because we wanted to show the significance of the character level data and its co-relation with the syllable/phoneme importance which is seen in the speech papers
--Paper includes method of putting the whole text as a bag of words model, the text is in the form of unigram character model. 
-- Find a sober way of saying that the approach in the paper is for DOCUMENTS, the features they use while predicting the languages are done by vector mapping, and thus alot of data is required to get proper probabilities. What the reviewer is suggesting that this approach can be used to predict code switching points.. What we have to show is that the twitter data or any other no standard data is small in size (140 char in case of tweets)

(2) Does the character language model include punctuation?   Or when you say
"space" do you mean "contiguous sequence of non-letter chars?"         What about
hyphens/apostrophes?

- More information to be provided in the section 3.1
- space -> Blank space
- punctuations and non letters are also part of the language model, we didn't mean to strip off any identity from the language being used in its original form

(3) In 3.3 you say "both of the word lists".  I learn later, in the evaluation
section, that the problem you've set for yourself assumes you know the two
languages involved in code-switching - you should lay this out more clearly at
the beginning of the paper, so section 3 can be read with that context in mind.

- In Introduction add the 
-cy - en 
-ga - en 
-br - fr 
Relation, Add citation to sources which talk about the high english/french influence in these regions, and why due to globalisation code-switching has been more prevalent.


(4) Fig 1 you have "a" chunked with cy following "black and tan"; is that the
bigram character model at work?  Heuristic alone would say it goes with the
preceding chunk, right?

- clarification from spectie 
- I think it was done from manual annotation, if it was manual, we can add the description in the figure



============================================================================
                            REVIEWER #2
============================================================================

(1) This paper presents a method for detecting code-switching subsegments for 3
pairs of languages. It was submitted as a short paper, but it is longer than
the 4 pages allowed by one paragraph. Although this was an interesting paper,
the writing was often unclear and I am not entirely sure that I have understood
everything. The authors should carefully read over any revised version of this
paper.

- Read over the paper and improve the language
- Repetition of a similar type of language in parts of my section ( ksnmi )
- Reduce the length of the paper after all the corrections are made

(2) The last paragraph of Section 2 contains incomplete text. "calculated them to
measure a confidence metric, to (Lyu et al., 2006)" does not make sense.

- Improve the referencing, not much here. 

(3) Why did you choose 1.5m words for the sample size? And why did you restrict the
English and French data to be the same size?

- Limiting size of the Breton data
- All the languages had to be of the same size to avoid bias
- Another Possibility - Character model results on 100k to 2 million.. A Graph to show that above 1 million the results are fairly stable. So our size was well suited    #suggestion needed (spectie)

(4) It would have been helpful to outline your goal again at the beginning of
Section 3.

- Will hopefully be solved after we have improved on the language 


(5) It seems that using standard wordlists might be restricting the upper bound of
performance, given the genre of the text: informal online writing. People use
all sorts of shorthand and acronyms that you will never find in a dictionary.
Do you have any way to identify informal words (e.g. an8 -- anocht) for
specific languages in advance (e.g. from tweets that contain only one of your
target languages)?

- Up for debate (spectie)
- We can solve this by mentioning about the good approach that it is and how people have used it before and bury the topic
- We can add abbreviations and other words we have into the wordlists and build another comparison test

(6) The sentence in the middle of Section 3.2 needs to be split into several
sentences. It is not at all clear in this section how you make the decisions
about labelling each word. You talk about doubt and confidence, how is this
measured? Are you using the character n-gram language models here? What
information is taken into account for each decision? Is it only the current
word? Or is it also the surrounding context, one one or both sides? It actually
seems that you have three methods for making these decisions described in
discontinuous subsections 3.1, 3.3 and 3.4. If this is the case, the structure
needs to be revised to reflect the natural grouping of these approaches.



- Better clarification of the placement of 3.1 and 3.2 .... 3.2 is being used as a sub part in many mechanisms and that has to be made clear.
- Maybe split the sentence or put them down as pseudo-algorithmic/point wise steps to better show what is happening here. (Saves space)

(7) In section 4, how were the tweets originally identified into the three
languages? Automatically or manually? And how does this work if they actually
contain English or French? Do you go by the majority language? How is that
defined?


- Since we are working with the hypothesis that the majority languages (ga-cy-br) use embedded languages (en-fr), we langid'd the tweets and got the sample data set, which was predicting the languages ( cy/br/ga )

(8) In 4.2 when you say you restricted the predicted languages to be only the two
in the dataset, firstly isn't this cheating? But second, what exactly are you
restricting? I'm guessing from the subsection title that it is the prediction
of the langid.py tool, but this is not clear.

- This is not cheating, in fact we are helping the competitive tool(langid) to narrow its probabilities to subsegment only two languages.
- We can use the probabilities of the languages being predicted by the tool, and say that if we don't restrict the set to just the known (2) languages it would not be good enough 
- Mention about the langid.py module which helps in restricting the ranks of the languages to be predicted


(9) 4.2 is very unclear. A 4 line sentence is just too long. "Some rules like in
Section 3 were followed": does this mean that you applied the rules that you
defined in Section 3 (if I understand correctly, there are three different
types)? Why "like", are they not identical?

- This problem will be solved if we address the point (6) in this review note and make changes accordingly here. Mention the exact changes, more specifically the ones related the the sentence chunking which are more closely imitated. 

- Re-write the sentence in a shorter form

(10) I am not sure how to interpret the labels in Tables 3 and 4 (which are never
mentioned in the text). You should mention these labels at the points in the
text where you are actually describing them. What is 'bigram'? Presumably it is
word rather than character bigrams, but it is never described what exactly you
do with bigrams. It is also not clear how you combine bigrams and the wordlist.
In 3.4 you talked about word based prediction with character backoff, but then
I don't know what the difference between langid2 and bigram is. And then where
would trigram be? This is very unclear.

- Add Legend description for the labels

(11) Footnote 4 seems like it requires further explanation. If there is a notion of
default models, does that also mean that one could train an alternative model?

"This could be because of the default models used" 

- More clarification after reading the usage of the toolkit and paper again
- We can be more specific here
- Also add that we can/cannot train on different models ( as far as i remember - we can )

(12) In Table 4, you give the accuracy for token language identification. It seems
that you already get pretty far using a wordlist, is this also knowing the
possible two languages in advance and only consulting those two wordlists?

- Talk more about the wordlist approach and how good it is doing 
- Points above will clarify the usage and the pre-knowledge of the languages


============================================================================
                            REVIEWER #3
============================================================================






(10) The first sentence in Section 5 is incomplete.
